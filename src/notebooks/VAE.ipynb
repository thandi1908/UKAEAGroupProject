{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb26f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scripts.utils import ScaleData, train_keys\n",
    "from scripts.AutoEncoder import EncoderBig, DecoderBig, AutoEncoderDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "#%matplotlib notebook\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7798e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63def6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(encoder, decoder, X):\n",
    "        # calculate the ELBO loss\n",
    "        q_z_given_x = encoder.forward(X)\n",
    "\n",
    "        q_samples = q_z_given_x.rsample()\n",
    "\n",
    "        ones = torch.ones(3)\n",
    "        zeros = torch.zeros(3)\n",
    "        \n",
    "#         if torch.cuda.is_available():\n",
    "#             ones.cuda()\n",
    "#             zeros.cuda()\n",
    "#             q_samples.cuda()\n",
    "        \n",
    "        latent_prior = dist.Normal(zeros, 3*ones)\n",
    "             \n",
    "        log_p_z = latent_prior.log_prob(q_samples).sum(-1)\n",
    "\n",
    "        log_q_z_given_x = q_z_given_x.log_prob(q_samples).sum(-1)\n",
    "\n",
    "        log_p_x_given_z = decoder.forward(q_samples).log_prob(X).sum(dim=1)\n",
    "        \n",
    "        ELBO = log_p_x_given_z + log_p_z - log_q_z_given_x\n",
    "\n",
    "        return ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3956a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderBig(15, 3, VAE = True)\n",
    "decoder = DecoderBig(15, 3, VAE = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a0a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/train_data_clipped.pkl\"\n",
    "train_data = AutoEncoderDataset(train_data_path, columns = train_keys, train = True)\n",
    "# train_data.data = train_data.data.sample(50_000)\n",
    "train_data.scale()\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = 1024)\n",
    "\n",
    "valid_data_path = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/valid_data_clipped.pkl\"\n",
    "valid_data = AutoEncoderDataset(valid_data_path, columns = train_keys, train = True)\n",
    "# valid_data.data = valid_data.data.sample(50_000)\n",
    "valid_data.scale()\n",
    "\n",
    "valid_loader = DataLoader(valid_data, shuffle = True, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb43d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  36%|███▌      | 9299/26090 [13:55<25:08, 11.13it/s]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1024, 3)) of distribution Normal(loc: torch.Size([1024, 3]), scale: torch.Size([1024, 3])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        ...,\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6100/571813317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mopt_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6100/1011599993.py\u001b[0m in \u001b[0;36mELBO\u001b[0;34m(encoder, decoder, X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m# calculate the ELBO loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mq_z_given_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mq_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_z_given_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UKAEAGroupProject/src/scripts/AutoEncoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;31m#return mu, logvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     56\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1024, 3)) of distribution Normal(loc: torch.Size([1024, 3]), scale: torch.Size([1024, 3])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan],\n        ...,\n        [nan, nan, nan],\n        [nan, nan, nan],\n        [nan, nan, nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     encoder = encoder.cuda()\n",
    "#     decoder = decoder.cuda() \n",
    "\n",
    "opt_vae = torch.optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters()), lr = 0.0001)\n",
    "N_epochs = 100 # Note that you may want to run more than 10 epochs!\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    train_loss = 0.0\n",
    "    for X in tqdm(train_loader, desc = f'Epoch {epoch}'):\n",
    "#         if torch.cuda.is_available():\n",
    "#             X = X.cuda()\n",
    "\n",
    "        opt_vae.zero_grad()\n",
    "        loss = -ELBO(encoder, decoder, X).mean()\n",
    "        loss.backward()\n",
    "        opt_vae.step()\n",
    "        train_loss += loss.item() * X.shape[0] / len(train_data)\n",
    "    print(\"Epoch %d, train loss = %0.4f\" % (epoch, train_loss));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_random_batch = next(iter(valid_loader))\n",
    "X_random_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec789570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    out = encoder(X_random_batch).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(out[:,0], out[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1005e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/test_data_clipped.pkl\"\n",
    "\n",
    "df_test = pd.read_pickle(test)\n",
    "df_test = df_test.sample(10_000)\n",
    "target = df_test['target']\n",
    "df_test_good = df_test[df_test.target == 1]\n",
    "df_test_good = df_test_good[train_keys]\n",
    "\n",
    "df_test_good,_ = ScaleData(df_test_good)\n",
    "\n",
    "df_test_bad = df_test[df_test.target == 0]\n",
    "df_test_bad = df_test_bad[train_keys]\n",
    "df_test_bad,_ = ScaleData(df_test_bad)\n",
    "\n",
    "df_test_good.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_good = torch.from_numpy(df_test_good.values).float()\n",
    "data_bad = torch.from_numpy(df_test_bad.values).float()\n",
    "#data_good_batch = next(iter(data_good_loader))\n",
    "with torch.no_grad():\n",
    "    outputs_good = encoder.forward(data_good).sample()\n",
    "    outputs_bad = encoder.forward(data_bad).sample()\n",
    "#    outputs_good = encoder.forward(data_good).sample().detach().numpy()\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(outputs_good[:,0], outputs_good[:,1])\n",
    "plt.scatter(outputs_bad[:,0], outputs_bad[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_output = decoder.forward(encoder.forward(data_good).sample()).sample().detach().numpy()\n",
    "df_ae_output = pd.DataFrame(AE_output, columns = train_keys)\n",
    "df_ae_output['AE'] = 'Outputs'\n",
    "\n",
    "df_test_tmp = df_test_good\n",
    "df_test_tmp['AE'] = 'Inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.concat([df_ae_output, df_test_tmp], ignore_index=True)\n",
    "df_compare_sample = df_compare.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4e89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in train_keys:\n",
    "    plt.figure()\n",
    "    x_min = df_compare_sample[i].quantile(0.1)\n",
    "    x_max = df_compare_sample[i].quantile(0.9)\n",
    "    sns.histplot(data = df_compare_sample, x = i, hue = \"AE\", binrange = (x_min, x_max), bins = 100);\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8a592",
   "metadata": {},
   "source": [
    "# VAE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1cfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearVAE, self).__init__()\n",
    "        \n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=15, out_features = 10)\n",
    "        self.enc2 = nn.Linear(in_features=10, out_features = 5)\n",
    "        \n",
    "        self.mu = nn.Linear(5, 2)\n",
    "        self.sigma = nn.Linear(5,2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features = 2, out_features = 5)\n",
    "        self.dec2 = nn.Linear(in_features = 5, out_features = 10)\n",
    "        self.dec3 = nn.Linear(10, 15)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = x.float()\n",
    "        x = F.relu(self.enc1(x.float()))\n",
    "        x = F.relu(self.enc2(x.float()))\n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x) # the first feature values as mean\n",
    "        log_var = self.sigma(x) # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        z = F.relu(self.dec1(z.float()))\n",
    "        z = F.relu(self.dec2(z.float()))\n",
    "        reconstruction = self.dec3(z.float())\n",
    "        return reconstruction.float(), mu.float(), log_var.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17374265",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.002\n",
    "epochs = 50\n",
    "\n",
    "model = LinearVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss(reduction = \"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(MSE_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = MSE_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #data, _ = data\n",
    "        #data = data.to(device)\n",
    "        #data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        MSE_loss = criterion(reconstruction.float(), data.float())\n",
    "        loss = final_loss(MSE_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(valid_data)/dataloader.batch_size)):\n",
    "            #data = data.to(device)\n",
    "            #data = data.view(data.size(0), -1)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            MSE_loss = criterion(reconstruction.float(), data.float())\n",
    "            loss = final_loss(MSE_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf09f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    val_epoch_loss = validate(model, valid_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45525a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_output2,_,_ = model(data_good)\n",
    "AE_output2 = AE_output2.detach().numpy()\n",
    "df_ae_output2 = pd.DataFrame(AE_output2, columns = train_keys)\n",
    "df_ae_output2['AE'] = 'Outputs'\n",
    "\n",
    "df_compare2 = pd.concat([df_ae_output2, df_test_tmp], ignore_index=True)\n",
    "df_compare_sample2= df_compare2.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff728fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in train_keys:\n",
    "    plt.figure()\n",
    "    x_min = df_compare_sample2[i].quantile(0.1)\n",
    "    x_max = df_compare_sample2[i].quantile(0.9)\n",
    "    sns.histplot(data = df_compare_sample2, x = i, hue = \"AE\", binrange = (x_min, x_max), bins = 100);\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6a3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
