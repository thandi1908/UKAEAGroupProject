{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb26f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scripts.utils import ScaleData, train_keys\n",
    "from scripts.AutoEncoder import Encoder, Decoder, AutoEncoderDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7798e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63def6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(encoder, decoder, X):\n",
    "        # calculate the ELBO loss\n",
    "        q_z_given_x = encoder.forward(X)\n",
    "\n",
    "        q_samples = q_z_given_x.rsample()\n",
    "\n",
    "        ones = torch.ones(2)\n",
    "        zeros = torch.zeros(2)\n",
    "        \n",
    "#         if torch.cuda.is_available():\n",
    "#             ones.cuda()\n",
    "#             zeros.cuda()\n",
    "#             q_samples.cuda()\n",
    "        \n",
    "        latent_prior = dist.Normal(zeros, ones)\n",
    "             \n",
    "        log_p_z = latent_prior.log_prob(q_samples).sum(-1)\n",
    "\n",
    "        log_q_z_given_x = q_z_given_x.log_prob(q_samples).sum(-1)\n",
    "\n",
    "        log_p_x_given_z = decoder.forward(q_samples).log_prob(X).sum(dim=1)\n",
    "        \n",
    "        ELBO = log_p_x_given_z + log_p_z - log_q_z_given_x\n",
    "\n",
    "        return ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(3, 15, VAE = True)\n",
    "decoder = Decoder(3, 15, VAE = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/train_data_clipped.pkl\"\n",
    "train_data = AutoEncoderDataset(train_data_path, columns = train_keys, train = True)\n",
    "train_data.data = train_data.data.sample(100_000)\n",
    "train_data.scale()\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle = True, batch_size = 2048)\n",
    "\n",
    "valid_data_path = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/valid_data_clipped.pkl\"\n",
    "valid_data = AutoEncoderDataset(valid_data_path, columns = train_keys, train = True)\n",
    "valid_data.data = valid_data.data.sample(100_000)\n",
    "valid_data.scale()\n",
    "\n",
    "valid_loader = DataLoader(valid_data, shuffle = True, batch_size = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     encoder = encoder.cuda()\n",
    "#     decoder = decoder.cuda() \n",
    "\n",
    "opt_vae = torch.optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters()))\n",
    "N_epochs = 50 # Note that you may want to run more than 10 epochs!\n",
    "for epoch in range(N_epochs):\n",
    "    train_loss = 0.0\n",
    "    for X in train_loader:\n",
    "#         if torch.cuda.is_available():\n",
    "#             X = X.cuda()\n",
    "\n",
    "        opt_vae.zero_grad()\n",
    "        loss = -ELBO(encoder, decoder, X).mean()\n",
    "        loss.backward()\n",
    "        opt_vae.step()\n",
    "        train_loss += loss.item() * X.shape[0] / len(train_data)\n",
    "    print(\"Epoch %d, train loss = %0.4f\" % (epoch, train_loss));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_random_batch = next(iter(valid_loader))\n",
    "X_random_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec789570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    out = encoder(X_random_batch).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(out[:,0], out[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1005e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"/share/rcifdata/jbarr/UKAEAGroupProject/data/test_data_clipped.pkl\"\n",
    "\n",
    "df_test = pd.read_pickle(test)\n",
    "df_test = df_test.sample(10_000)\n",
    "target = df_test['target']\n",
    "df_test_good = df_test[df_test.target == 1]\n",
    "df_test_good = df_test_good[train_keys]\n",
    "\n",
    "df_test_good,_ = ScaleData(df_test_good)\n",
    "\n",
    "df_test_bad = df_test[df_test.target == 0]\n",
    "df_test_bad = df_test_bad[train_keys]\n",
    "df_test_bad,_ = ScaleData(df_test_bad)\n",
    "\n",
    "df_test_good.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_good = torch.from_numpy(df_test_good.values).float()\n",
    "data_bad = torch.from_numpy(df_test_bad.values).float()\n",
    "#data_good_batch = next(iter(data_good_loader))\n",
    "with torch.no_grad():\n",
    "    outputs_good = encoder.forward(data_good).sample()\n",
    "    outputs_bad = encoder.forward(data_bad).sample()\n",
    "#    outputs_good = encoder.forward(data_good).sample().detach().numpy()\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter(outputs_good[:,0], outputs_good[:,1])\n",
    "plt.scatter(outputs_bad[:,0], outputs_bad[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_output = decoder.forward(encoder.forward(data_good).sample()).sample().detach().numpy()\n",
    "df_ae_output = pd.DataFrame(AE_output, columns = train_keys)\n",
    "df_ae_output['AE'] = 'Outputs'\n",
    "\n",
    "df_test_tmp = df_test_good\n",
    "df_test_tmp['AE'] = 'Inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.concat([df_ae_output, df_test_tmp], ignore_index=True)\n",
    "df_compare_sample = df_compare.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4e89d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in train_keys:\n",
    "    plt.figure()\n",
    "    x_min = df_compare_sample[i].quantile(0.1)\n",
    "    x_max = df_compare_sample[i].quantile(0.9)\n",
    "    sns.histplot(data = df_compare_sample, x = i, hue = \"AE\", binrange = (x_min, x_max), bins = 100);\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2fd17",
   "metadata": {},
   "source": [
    "# VAE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd45c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearVAE, self).__init__()\n",
    "        \n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=15, out_features = 10)\n",
    "        self.enc2 = nn.Linear(in_features=10, out_features = 5)\n",
    "        \n",
    "        self.mu = nn.Linear(5, 2)\n",
    "        self.sigma = nn.Linear(5,2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features = 2, out_features = 5)\n",
    "        self.dec2 = nn.Linear(in_features = 5, out_features = 10)\n",
    "        self.dec3 = nn.Linear(10, 15)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = x.float()\n",
    "        x = F.relu(self.enc1(x.float()))\n",
    "        x = F.relu(self.enc2(x.float()))\n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x) # the first feature values as mean\n",
    "        log_var = self.sigma(x) # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        z = F.relu(self.dec1(z.float()))\n",
    "        z = F.relu(self.dec2(z.float()))\n",
    "        reconstruction = self.dec3(z.float())\n",
    "        return reconstruction.float(), mu.float(), log_var.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "model = LinearVAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7538fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5de989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #data, _ = data\n",
    "        #data = data.to(device)\n",
    "        #data = data.view(data.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction.float(), data.float())\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e537bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(valid_data)/dataloader.batch_size)):\n",
    "            #data = data.to(device)\n",
    "            #data = data.view(data.size(0), -1)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction.float(), data.float())\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9817928",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    val_epoch_loss = validate(model, valid_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f112fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_output2,_,_ = model(data_good)\n",
    "AE_output2 = AE_output2.detach().numpy()\n",
    "df_ae_output2 = pd.DataFrame(AE_output2, columns = train_keys)\n",
    "df_ae_output2['AE'] = 'Outputs'\n",
    "\n",
    "df_compare2 = pd.concat([df_ae_output2, df_test_tmp], ignore_index=True)\n",
    "df_compare_sample2= df_compare2.sample(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810dbbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in train_keys:\n",
    "    plt.figure()\n",
    "    x_min = df_compare_sample2[i].quantile(0.1)\n",
    "    x_max = df_compare_sample2[i].quantile(0.9)\n",
    "    sns.histplot(data = df_compare_sample2, x = i, hue = \"AE\", binrange = (x_min, x_max), bins = 100);\n",
    "    plt.xlabel(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b02b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
