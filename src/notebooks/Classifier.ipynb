{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33210b17",
   "metadata": {},
   "source": [
    "### Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scripts.utils import train_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2cf0fb",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle(\"/share/rcifdata/jbarr/UKAEAGroupProject/data/train_data_clipped.pkl\")\n",
    "\n",
    "X_train, Y_train = train_data.iloc[:,:15].to_numpy(), train_data.iloc[:,-1].to_numpy()\n",
    "\n",
    "validation_data = pd.read_pickle(\"/share/rcifdata/jbarr/UKAEAGroupProject/data/valid_data_clipped.pkl\")\n",
    "\n",
    "X_val, Y_val = validation_data.iloc[:,:15].to_numpy(), validation_data.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251d2185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26715960, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = X_train.corr()\n",
    "\n",
    "<<<<<<< local\n",
    "# plt.figure(figsize = (12,8))\n",
    "# sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, linewidths = .5, cmap = \"hot\")\n",
    "=======\n",
    "#plt.figure(figsize = (12,8))\n",
    "#sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns, linewidths = .5, cmap = \"hot\")\n",
    ">>>>>>> remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "x_train = scaler.transform(X_train)\n",
    "x_val = scaler.transform (X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406bbaa",
   "metadata": {},
   "source": [
    "###  Best Neural Network Classifier From Initial Grid Search\n",
    "\n",
    "**Initial Grid Search Parameters**\n",
    "\n",
    "Number of nodes: [5,10, 20, 30]\n",
    "\n",
    "Number of layers: [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/share/rcifdata/jbarr/UKAEAGroupProject/grid_search/'\n",
    "trainings = []\n",
    "for i in range(336):\n",
    "    trial_dict = pickle.load(open(file_path+\"trial_\"+str(i)+\".pkl\", 'rb'))\n",
    "    trainings.append(trial_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8625509",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate the number of parameters\n",
    "def model_params(nodes, n_inputs=15):\n",
    "    nodes.append(1)\n",
    "    params = 0\n",
    "    \n",
    "    # bottle neck, 0 if model doesn't include a bottle neck and 1 if the model does include one.\n",
    "    bottle_in = 0\n",
    "    bottle_hidden = 0\n",
    "    \n",
    "    # parameters from models\n",
    "    for i in range(len(nodes)):\n",
    "        if i == 0:\n",
    "            if nodes[i] < n_inputs: bottle_in = 1\n",
    "            params += n_inputs * nodes[i]\n",
    "        else:\n",
    "            params += nodes[i-1]* nodes[i]\n",
    "            \n",
    "            if nodes[i-1] < nodes[i]: bottle_hidden = 1\n",
    "    \n",
    "    # parameters from biases\n",
    "    for i in nodes: \n",
    "        params += i\n",
    "    return params, bottle_in, bottle_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_params([2,2])[0] == 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best architecture\n",
    "best_trial = None \n",
    "best_val_acc = - sys.float_info.max\n",
    "val_accs = []\n",
    "n_params = []\n",
    "bottle_in = []\n",
    "bottle_hid = []\n",
    "\n",
    "for trial in trainings:\n",
    "    val_acc = trial[\"perfomance\"][1]\n",
    "    \n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    n_param, bot_in, bot_hid  = model_params(list(trial['nodes']))\n",
    "    \n",
    "    n_params.append(n_param)\n",
    "    \n",
    "    bottle_in.append(bot_in)\n",
    "    bottle_hid.append(bot_hid)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if val_acc > best_val_acc: \n",
    "        best_val_acc = val_acc\n",
    "        best_trial = trial\n",
    "        \n",
    "n_params = np.array(n_params)\n",
    "bottle_in = np.array(bottle_in)\n",
    "bottle_hid= np.array(bottle_hid)\n",
    "val_accs = np.array(val_accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Model\\n\")\n",
    "print(\"Network:\",best_trial[\"nodes\"],\"\\n\" )\n",
    "print(\"Validation accuracy:\", best_trial['perfomance'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(best_trial[\"history\"][\"acc\"],color = 'blue')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(best_trial[\"history\"][\"loss\"], color = 'blue')\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b26d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(val_accs, bins = 20, color = 'purple');\n",
    "plt.xlabel(\"Validation Accuracy\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a7a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: text/plain>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb3948ceeb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <removed>\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(n_params[np.where(bottle_in == 0)], val_accs[np.where(bottle_in == 0)], '.', label = 'Normal')\n",
    "plt.plot(n_params[np.where(bottle_in == 1)], val_accs[np.where(bottle_in == 1)], '.', label = 'Input bn')\n",
    "plt.plot(n_params[np.where(bottle_hid == 1)], val_accs[np.where(bottle_hid == 1)], '.', label = 'Hidden bn')\n",
    "plt.xlabel(\"Number of Model Parameters\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18029aa4",
   "metadata": {},
   "source": [
    "### Grid Search Conclusion\n",
    "\n",
    "Looking at the plot above we can see that increasing the number of model parameters in general leads to an increase in the models validation accuracy, the graph looks as though it will plateau, but it also suggestes we haven't hit the peak, a further grid search of models with a higher number of parameters should be conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a2ce8",
   "metadata": {},
   "source": [
    "### Final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classifier():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(30, activation = 'tanh'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')   \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 18:35:10.917435: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-03-10 18:35:10.918176: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 18:35:10.918331: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <removed>\n"
     ]
    }
   ],
   "source": [
    "model = nn_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de731df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = 'acc')\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45621c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: >\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 18:35:33.798586: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-03-10 18:35:33.813404: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2200000000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <removed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6523/6523 [==============================] - 347s 51ms/step - loss: 0.4837 - acc: 0.7460 - val_loss: 0.3819 - val_acc: 0.8157\n",
      "Epoch 2/50\n",
      "6523/6523 [==============================] - 275s 42ms/step - loss: 0.4049 - acc: 0.8011 - val_loss: 0.3720 - val_acc: 0.8217\n",
      "Epoch 3/50\n",
      "6523/6523 [==============================] - 252s 39ms/step - loss: 0.3969 - acc: 0.8060 - val_loss: 0.3682 - val_acc: 0.8240\n",
      "Epoch 4/50\n",
      "3928/6523 [=================>............] - ETA: 1:42 - loss: 0.3938 - acc: 0.8080"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55103/597723551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, Y_train, validation_data = (x_val, Y_val), batch_size = 4096, epochs =50, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p saved_model\n",
    "# model.save('saved_model/classifier_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ac323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['acc'], 'o', label = 'Train acc')\n",
    "plt.plot(history.history['val_acc'], 'o', label = 'Val acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56127d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'], 'o', label = 'Train loss')\n",
    "plt.plot(history.history['val_loss'], 'o', label = 'Val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398f7d9",
   "metadata": {},
   "source": [
    "### Evaluate Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98607833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /share/rcifdata/jbarr/UKAEAGroupProject/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25260b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(\"/share/rcifdata/jbarr/UKAEAGroupProject/data/test_data_clipped.pkl\")\n",
    "\n",
    "X_test, Y_test = test_data.iloc[:,:15].to_numpy(), test_data.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f92687",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "random_class = np.arange(0,1,0.005)\n",
    "plt.plot(random_class, random_class, '--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('Flase Negative Rate')\n",
    "plt.text(0.8, 0.2, f'auc = {auc: .2f}', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.where(predictions < 0.5, predictions, 1)\n",
    "test_pred = np.where(predictions >= 0.5, test_pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86457a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = np.where(Y_test ==0)[0].shape[0]\n",
    "n_negative = np.where(Y_test ==1)[0].shape[0]\n",
    "div_arr = np.array([[n_positive, n_negative]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12713400",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = confusion_matrix(Y_test, test_pred, labels =[0,1])/div_arr\n",
    "sns.heatmap(con_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73e3fd",
   "metadata": {},
   "source": [
    "### Distributions from classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.round(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d485338",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_output = x_test[np.where(preds == 0)]\n",
    "yes_output = x_test[np.where(preds == 1)] \n",
    "assert no_output.shape[0] + yes_output.shape[0] == x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96807ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train_data.iloc[:,:-1].columns)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb7b4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, column in enumerate(columns):\n",
    "    # find the mean and std \n",
    "    mean_no, mean_yes = np.mean(no_output[:,i]), np.mean(yes_output[:,i])\n",
    "    std_no, std_yes = np.std(no_output[:,i]), np.std(yes_output[:,i])\n",
    "    \n",
    "    no_lower, no_upper = (mean_no - 3*std_no), (mean_no + 3*std_no)\n",
    "    yes_lower, yes_upper = (mean_yes - 3*std_yes), (mean_yes + 3*std_yes)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(no_output[:,i], histtype = 'step', color = 'lime',\n",
    "             label =\" No output\", density = True, bins =50, range = (no_lower, no_upper));\n",
    "    \n",
    "    plt.hist(yes_output[:,i], histtype = 'step', color = 'purple',\n",
    "             label = \"Output\", density = True, bins = 50, range = (yes_lower,yes_upper ));\n",
    "    plt.legend()\n",
    "    plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298e5c4",
   "metadata": {},
   "source": [
    "# Train model with varying number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b368b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_classifier_big():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256,activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')   \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# def nn_classifier_big():\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(1, activation = 'sigmoid')   \n",
    "#     ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< REMOTE CELL DELETED >>>>>>>\n",
    "# def nn_classifier_big():\n",
    "#     model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "#     tf.keras.layers.Dropout(0.1),\n",
    "#     tf.keras.layers.Dense(1, activation = 'sigmoid')   \n",
    "#     ])\n",
    "#     return model\n",
    "def nn_classifier_big():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(150, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(75, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(30, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')   \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = np.random.choice(x_val.shape[0], size = 100_000, replace=False)\n",
    "x_val_sample = x_val[val_indices]\n",
    "y_val_sample = Y_val[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec988372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 1000 training points:\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6898 - acc: 0.5500 - val_loss: 0.6714 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6667 - acc: 0.6660 - val_loss: 0.6564 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6468 - acc: 0.6660 - val_loss: 0.6449 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6440 - acc: 0.6660 - val_loss: 0.6338 - val_acc: 0.6604\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6321 - acc: 0.6660 - val_loss: 0.6214 - val_acc: 0.6604\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6154 - acc: 0.6660 - val_loss: 0.6111 - val_acc: 0.6604\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6013 - acc: 0.6660 - val_loss: 0.6029 - val_acc: 0.6604\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5923 - acc: 0.6660 - val_loss: 0.5962 - val_acc: 0.6604\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5838 - acc: 0.6660 - val_loss: 0.5916 - val_acc: 0.6604\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5724 - acc: 0.6660 - val_loss: 0.5897 - val_acc: 0.6604\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5689 - acc: 0.6660 - val_loss: 0.5895 - val_acc: 0.6604\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5673 - acc: 0.6660 - val_loss: 0.5871 - val_acc: 0.6604\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5666 - acc: 0.6660 - val_loss: 0.5831 - val_acc: 0.6604\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5566 - acc: 0.6660 - val_loss: 0.5805 - val_acc: 0.6604\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5615 - acc: 0.6660 - val_loss: 0.5783 - val_acc: 0.6606\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5565 - acc: 0.6670 - val_loss: 0.5772 - val_acc: 0.6623\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5549 - acc: 0.6660 - val_loss: 0.5769 - val_acc: 0.6636\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5503 - acc: 0.6680 - val_loss: 0.5775 - val_acc: 0.6631\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5455 - acc: 0.6670 - val_loss: 0.5783 - val_acc: 0.6639\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5493 - acc: 0.6700 - val_loss: 0.5781 - val_acc: 0.6658\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5398 - acc: 0.6690 - val_loss: 0.5771 - val_acc: 0.6698\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5405 - acc: 0.6800 - val_loss: 0.5761 - val_acc: 0.6720\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5380 - acc: 0.6710 - val_loss: 0.5757 - val_acc: 0.6722\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5349 - acc: 0.6710 - val_loss: 0.5762 - val_acc: 0.6718\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5385 - acc: 0.6810 - val_loss: 0.5760 - val_acc: 0.6719\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5379 - acc: 0.6850 - val_loss: 0.5753 - val_acc: 0.6723\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5364 - acc: 0.6900 - val_loss: 0.5748 - val_acc: 0.6725\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5236 - acc: 0.6970 - val_loss: 0.5755 - val_acc: 0.6719\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5317 - acc: 0.7000 - val_loss: 0.5762 - val_acc: 0.6703\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5196 - acc: 0.7110 - val_loss: 0.5789 - val_acc: 0.6700\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5225 - acc: 0.7100 - val_loss: 0.5806 - val_acc: 0.6690\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5154 - acc: 0.7060 - val_loss: 0.5800 - val_acc: 0.6678\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5156 - acc: 0.7200 - val_loss: 0.5783 - val_acc: 0.6662\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5096 - acc: 0.7180 - val_loss: 0.5788 - val_acc: 0.6668\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5094 - acc: 0.7110 - val_loss: 0.5815 - val_acc: 0.6680\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5083 - acc: 0.7190 - val_loss: 0.5846 - val_acc: 0.6706\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5128 - acc: 0.7140 - val_loss: 0.5876 - val_acc: 0.6719\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5057 - acc: 0.7290 - val_loss: 0.5890 - val_acc: 0.6735\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5067 - acc: 0.7240 - val_loss: 0.5858 - val_acc: 0.6741\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4994 - acc: 0.7350 - val_loss: 0.5825 - val_acc: 0.6750\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5023 - acc: 0.7300 - val_loss: 0.5799 - val_acc: 0.6759\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4918 - acc: 0.7320 - val_loss: 0.5780 - val_acc: 0.6753\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4934 - acc: 0.7520 - val_loss: 0.5793 - val_acc: 0.6775\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4863 - acc: 0.7480 - val_loss: 0.5818 - val_acc: 0.6800\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4766 - acc: 0.7630 - val_loss: 0.5843 - val_acc: 0.6811\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4860 - acc: 0.7560 - val_loss: 0.5843 - val_acc: 0.6813\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4906 - acc: 0.7500 - val_loss: 0.5812 - val_acc: 0.6799\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4796 - acc: 0.7630 - val_loss: 0.5829 - val_acc: 0.6788\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4826 - acc: 0.7520 - val_loss: 0.5864 - val_acc: 0.6834\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4727 - acc: 0.7520 - val_loss: 0.5917 - val_acc: 0.6862\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4760 - acc: 0.7610 - val_loss: 0.5921 - val_acc: 0.6862\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4659 - acc: 0.7640 - val_loss: 0.5883 - val_acc: 0.6824\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4771 - acc: 0.7550 - val_loss: 0.5870 - val_acc: 0.6807\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4746 - acc: 0.7490 - val_loss: 0.5892 - val_acc: 0.6870\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4558 - acc: 0.7730 - val_loss: 0.5967 - val_acc: 0.6899\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4565 - acc: 0.7840 - val_loss: 0.5932 - val_acc: 0.6898\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4392 - acc: 0.7850 - val_loss: 0.5929 - val_acc: 0.6862\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4525 - acc: 0.7770 - val_loss: 0.5975 - val_acc: 0.6848\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4517 - acc: 0.7570 - val_loss: 0.6020 - val_acc: 0.6893\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4358 - acc: 0.7850 - val_loss: 0.6123 - val_acc: 0.6944\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4341 - acc: 0.7760 - val_loss: 0.6171 - val_acc: 0.6951\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4541 - acc: 0.7760 - val_loss: 0.6085 - val_acc: 0.6931\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4457 - acc: 0.7770 - val_loss: 0.6016 - val_acc: 0.6913\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.4351 - acc: 0.7910 - val_loss: 0.5988 - val_acc: 0.6919\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4274 - acc: 0.7750 - val_loss: 0.6013 - val_acc: 0.6942\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4331 - acc: 0.7860 - val_loss: 0.6086 - val_acc: 0.6955\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4428 - acc: 0.7890 - val_loss: 0.6132 - val_acc: 0.6944\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4277 - acc: 0.7820 - val_loss: 0.6179 - val_acc: 0.6916\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4094 - acc: 0.8040 - val_loss: 0.6266 - val_acc: 0.6906\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4147 - acc: 0.8120 - val_loss: 0.6342 - val_acc: 0.6941\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4111 - acc: 0.7940 - val_loss: 0.6409 - val_acc: 0.6960\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4122 - acc: 0.8130 - val_loss: 0.6449 - val_acc: 0.6948\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4036 - acc: 0.8030 - val_loss: 0.6487 - val_acc: 0.6931\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4007 - acc: 0.8030 - val_loss: 0.6516 - val_acc: 0.6927\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4089 - acc: 0.7940 - val_loss: 0.6516 - val_acc: 0.6965\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3921 - acc: 0.8130 - val_loss: 0.6552 - val_acc: 0.6982\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3866 - acc: 0.8160 - val_loss: 0.6590 - val_acc: 0.6970\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3979 - acc: 0.8110 - val_loss: 0.6586 - val_acc: 0.6970\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3840 - acc: 0.8250 - val_loss: 0.6576 - val_acc: 0.6999\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3832 - acc: 0.8270 - val_loss: 0.6628 - val_acc: 0.7011\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3983 - acc: 0.8240 - val_loss: 0.6635 - val_acc: 0.6988\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3811 - acc: 0.8220 - val_loss: 0.6672 - val_acc: 0.6972\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3946 - acc: 0.8150 - val_loss: 0.6694 - val_acc: 0.7013\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3794 - acc: 0.8260 - val_loss: 0.6811 - val_acc: 0.7004\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3885 - acc: 0.8170 - val_loss: 0.6836 - val_acc: 0.6985\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3696 - acc: 0.8350 - val_loss: 0.6783 - val_acc: 0.6974\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3921 - acc: 0.8150 - val_loss: 0.6720 - val_acc: 0.6980\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3611 - acc: 0.8370 - val_loss: 0.6755 - val_acc: 0.6985\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3753 - acc: 0.8280 - val_loss: 0.6819 - val_acc: 0.6970\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3809 - acc: 0.8130 - val_loss: 0.6828 - val_acc: 0.6971\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3792 - acc: 0.8300 - val_loss: 0.6827 - val_acc: 0.6998\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3543 - acc: 0.8440 - val_loss: 0.6875 - val_acc: 0.6998\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3683 - acc: 0.8300 - val_loss: 0.6988 - val_acc: 0.6993\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3622 - acc: 0.8430 - val_loss: 0.7108 - val_acc: 0.6989\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3694 - acc: 0.8280 - val_loss: 0.7152 - val_acc: 0.6999\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3686 - acc: 0.8380 - val_loss: 0.7142 - val_acc: 0.7003\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3508 - acc: 0.8470 - val_loss: 0.7193 - val_acc: 0.7004\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3315 - acc: 0.8530 - val_loss: 0.7326 - val_acc: 0.6986\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3578 - acc: 0.8360 - val_loss: 0.7358 - val_acc: 0.7010\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3457 - acc: 0.8400 - val_loss: 0.7458 - val_acc: 0.7027\n",
      "\n",
      " \n",
      "\n",
      "Training model with 2000 training points:\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6888 - acc: 0.6015 - val_loss: 0.6706 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6622 - acc: 0.6715 - val_loss: 0.6565 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6497 - acc: 0.6715 - val_loss: 0.6480 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6411 - acc: 0.6715 - val_loss: 0.6366 - val_acc: 0.6604\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6354 - acc: 0.6715 - val_loss: 0.6250 - val_acc: 0.6604\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6186 - acc: 0.6715 - val_loss: 0.6173 - val_acc: 0.6604\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6127 - acc: 0.6715 - val_loss: 0.6106 - val_acc: 0.6604\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6029 - acc: 0.6715 - val_loss: 0.6030 - val_acc: 0.6604\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5944 - acc: 0.6715 - val_loss: 0.5975 - val_acc: 0.6604\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5868 - acc: 0.6715 - val_loss: 0.5956 - val_acc: 0.6604\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5825 - acc: 0.6715 - val_loss: 0.5940 - val_acc: 0.6604\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5791 - acc: 0.6715 - val_loss: 0.5893 - val_acc: 0.6604\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5780 - acc: 0.6715 - val_loss: 0.5841 - val_acc: 0.6604\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5703 - acc: 0.6715 - val_loss: 0.5809 - val_acc: 0.6604\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5670 - acc: 0.6715 - val_loss: 0.5790 - val_acc: 0.6604\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5652 - acc: 0.6715 - val_loss: 0.5790 - val_acc: 0.6604\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5621 - acc: 0.6720 - val_loss: 0.5820 - val_acc: 0.6604\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5604 - acc: 0.6715 - val_loss: 0.5849 - val_acc: 0.6604\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5650 - acc: 0.6715 - val_loss: 0.5836 - val_acc: 0.6604\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5598 - acc: 0.6725 - val_loss: 0.5792 - val_acc: 0.6608\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5591 - acc: 0.6720 - val_loss: 0.5755 - val_acc: 0.6616\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5544 - acc: 0.6715 - val_loss: 0.5740 - val_acc: 0.6626\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5511 - acc: 0.6735 - val_loss: 0.5741 - val_acc: 0.6632\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5524 - acc: 0.6715 - val_loss: 0.5755 - val_acc: 0.6639\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5495 - acc: 0.6765 - val_loss: 0.5772 - val_acc: 0.6654\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5536 - acc: 0.6745 - val_loss: 0.5749 - val_acc: 0.6686\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.5482 - acc: 0.6795 - val_loss: 0.5722 - val_acc: 0.6719\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5438 - acc: 0.6880 - val_loss: 0.5708 - val_acc: 0.6730\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5411 - acc: 0.6905 - val_loss: 0.5708 - val_acc: 0.6727\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5426 - acc: 0.6845 - val_loss: 0.5715 - val_acc: 0.6733\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5423 - acc: 0.6920 - val_loss: 0.5710 - val_acc: 0.6726\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5404 - acc: 0.6980 - val_loss: 0.5701 - val_acc: 0.6734\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5404 - acc: 0.6935 - val_loss: 0.5687 - val_acc: 0.6738\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5320 - acc: 0.7025 - val_loss: 0.5686 - val_acc: 0.6752\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5340 - acc: 0.7005 - val_loss: 0.5683 - val_acc: 0.6766\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5312 - acc: 0.7035 - val_loss: 0.5691 - val_acc: 0.6783\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5329 - acc: 0.7005 - val_loss: 0.5691 - val_acc: 0.6791\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5319 - acc: 0.7040 - val_loss: 0.5679 - val_acc: 0.6800\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5252 - acc: 0.7135 - val_loss: 0.5663 - val_acc: 0.6813\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5224 - acc: 0.7105 - val_loss: 0.5662 - val_acc: 0.6843\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5195 - acc: 0.7190 - val_loss: 0.5662 - val_acc: 0.6869\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5227 - acc: 0.7230 - val_loss: 0.5630 - val_acc: 0.6892\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5220 - acc: 0.7120 - val_loss: 0.5583 - val_acc: 0.6906\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5134 - acc: 0.7185 - val_loss: 0.5545 - val_acc: 0.6930\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5110 - acc: 0.7210 - val_loss: 0.5531 - val_acc: 0.6956\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5136 - acc: 0.7260 - val_loss: 0.5545 - val_acc: 0.6976\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5117 - acc: 0.7305 - val_loss: 0.5566 - val_acc: 0.7001\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5141 - acc: 0.7210 - val_loss: 0.5536 - val_acc: 0.7012\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5094 - acc: 0.7215 - val_loss: 0.5504 - val_acc: 0.7004\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5091 - acc: 0.7270 - val_loss: 0.5499 - val_acc: 0.7003\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5050 - acc: 0.7310 - val_loss: 0.5512 - val_acc: 0.7020\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5044 - acc: 0.7270 - val_loss: 0.5530 - val_acc: 0.7040\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5023 - acc: 0.7315 - val_loss: 0.5530 - val_acc: 0.7047\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5010 - acc: 0.7355 - val_loss: 0.5501 - val_acc: 0.7055\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5030 - acc: 0.7335 - val_loss: 0.5478 - val_acc: 0.7049\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4958 - acc: 0.7375 - val_loss: 0.5474 - val_acc: 0.7062\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4963 - acc: 0.7430 - val_loss: 0.5514 - val_acc: 0.7088\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4923 - acc: 0.7470 - val_loss: 0.5537 - val_acc: 0.7094\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4992 - acc: 0.7330 - val_loss: 0.5497 - val_acc: 0.7096\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4846 - acc: 0.7490 - val_loss: 0.5485 - val_acc: 0.7073\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4903 - acc: 0.7420 - val_loss: 0.5503 - val_acc: 0.7076\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4862 - acc: 0.7500 - val_loss: 0.5559 - val_acc: 0.7103\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4851 - acc: 0.7445 - val_loss: 0.5617 - val_acc: 0.7115\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4945 - acc: 0.7435 - val_loss: 0.5576 - val_acc: 0.7120\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4841 - acc: 0.7475 - val_loss: 0.5525 - val_acc: 0.7125\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4795 - acc: 0.7505 - val_loss: 0.5508 - val_acc: 0.7126\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4795 - acc: 0.7630 - val_loss: 0.5520 - val_acc: 0.7143\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4813 - acc: 0.7625 - val_loss: 0.5556 - val_acc: 0.7152\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4742 - acc: 0.7540 - val_loss: 0.5584 - val_acc: 0.7146\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4799 - acc: 0.7595 - val_loss: 0.5582 - val_acc: 0.7149\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4685 - acc: 0.7620 - val_loss: 0.5587 - val_acc: 0.7132\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4727 - acc: 0.7540 - val_loss: 0.5617 - val_acc: 0.7131\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4625 - acc: 0.7625 - val_loss: 0.5700 - val_acc: 0.7140\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4616 - acc: 0.7725 - val_loss: 0.5767 - val_acc: 0.7139\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4628 - acc: 0.7670 - val_loss: 0.5689 - val_acc: 0.7151\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4611 - acc: 0.7690 - val_loss: 0.5617 - val_acc: 0.7130\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4623 - acc: 0.7700 - val_loss: 0.5578 - val_acc: 0.7151\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4541 - acc: 0.7755 - val_loss: 0.5619 - val_acc: 0.7175\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4623 - acc: 0.7655 - val_loss: 0.5604 - val_acc: 0.7175\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4597 - acc: 0.7655 - val_loss: 0.5522 - val_acc: 0.7178\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4506 - acc: 0.7775 - val_loss: 0.5492 - val_acc: 0.7178\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4544 - acc: 0.7785 - val_loss: 0.5531 - val_acc: 0.7181\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4482 - acc: 0.7830 - val_loss: 0.5601 - val_acc: 0.7182\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4530 - acc: 0.7730 - val_loss: 0.5669 - val_acc: 0.7181\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4532 - acc: 0.7785 - val_loss: 0.5692 - val_acc: 0.7179\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4433 - acc: 0.7840 - val_loss: 0.5706 - val_acc: 0.7182\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4475 - acc: 0.7670 - val_loss: 0.5659 - val_acc: 0.7170\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4538 - acc: 0.7845 - val_loss: 0.5646 - val_acc: 0.7183\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4429 - acc: 0.7860 - val_loss: 0.5663 - val_acc: 0.7193\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.4439 - acc: 0.7825 - val_loss: 0.5632 - val_acc: 0.7195\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4441 - acc: 0.7840 - val_loss: 0.5612 - val_acc: 0.7186\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4426 - acc: 0.7820 - val_loss: 0.5656 - val_acc: 0.7187\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4413 - acc: 0.7760 - val_loss: 0.5735 - val_acc: 0.7196\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4382 - acc: 0.7885 - val_loss: 0.5735 - val_acc: 0.7200\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4364 - acc: 0.7845 - val_loss: 0.5713 - val_acc: 0.7192\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4280 - acc: 0.7920 - val_loss: 0.5733 - val_acc: 0.7171\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4296 - acc: 0.7975 - val_loss: 0.5764 - val_acc: 0.7177\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4362 - acc: 0.7785 - val_loss: 0.5809 - val_acc: 0.7194\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4285 - acc: 0.7865 - val_loss: 0.5818 - val_acc: 0.7193\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4187 - acc: 0.7950 - val_loss: 0.5838 - val_acc: 0.7192\n",
      "\n",
      " \n",
      "\n",
      "Training model with 5000 training points:\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6899 - acc: 0.5898 - val_loss: 0.6726 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6701 - acc: 0.6654 - val_loss: 0.6540 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6564 - acc: 0.6654 - val_loss: 0.6387 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6482 - acc: 0.6654 - val_loss: 0.6258 - val_acc: 0.6604\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6360 - acc: 0.6654 - val_loss: 0.6149 - val_acc: 0.6604\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6229 - acc: 0.6654 - val_loss: 0.6065 - val_acc: 0.6604\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6121 - acc: 0.6654 - val_loss: 0.5991 - val_acc: 0.6604\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6017 - acc: 0.6654 - val_loss: 0.5933 - val_acc: 0.6605\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5961 - acc: 0.6654 - val_loss: 0.5903 - val_acc: 0.6628\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5929 - acc: 0.6662 - val_loss: 0.5868 - val_acc: 0.6650\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5869 - acc: 0.6670 - val_loss: 0.5818 - val_acc: 0.6656\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5852 - acc: 0.6728 - val_loss: 0.5768 - val_acc: 0.6649\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5779 - acc: 0.6722 - val_loss: 0.5731 - val_acc: 0.6650\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5745 - acc: 0.6744 - val_loss: 0.5715 - val_acc: 0.6669\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5719 - acc: 0.6740 - val_loss: 0.5724 - val_acc: 0.6669\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5705 - acc: 0.6796 - val_loss: 0.5730 - val_acc: 0.6682\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5663 - acc: 0.6780 - val_loss: 0.5720 - val_acc: 0.6692\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5695 - acc: 0.6780 - val_loss: 0.5694 - val_acc: 0.6696\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5647 - acc: 0.6836 - val_loss: 0.5669 - val_acc: 0.6704\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5614 - acc: 0.6800 - val_loss: 0.5649 - val_acc: 0.6710\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5583 - acc: 0.6874 - val_loss: 0.5645 - val_acc: 0.6714\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5559 - acc: 0.6868 - val_loss: 0.5640 - val_acc: 0.6721\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5571 - acc: 0.6878 - val_loss: 0.5621 - val_acc: 0.6742\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5554 - acc: 0.6860 - val_loss: 0.5597 - val_acc: 0.6751\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5546 - acc: 0.6874 - val_loss: 0.5583 - val_acc: 0.6765\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5533 - acc: 0.6856 - val_loss: 0.5573 - val_acc: 0.6782\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5464 - acc: 0.6852 - val_loss: 0.5568 - val_acc: 0.6791\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5491 - acc: 0.6900 - val_loss: 0.5564 - val_acc: 0.6814\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5452 - acc: 0.6958 - val_loss: 0.5548 - val_acc: 0.6834\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5410 - acc: 0.6998 - val_loss: 0.5525 - val_acc: 0.6868\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5424 - acc: 0.6922 - val_loss: 0.5505 - val_acc: 0.6917\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5406 - acc: 0.7008 - val_loss: 0.5498 - val_acc: 0.6932\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5404 - acc: 0.7058 - val_loss: 0.5495 - val_acc: 0.6955\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5367 - acc: 0.7004 - val_loss: 0.5496 - val_acc: 0.6970\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5383 - acc: 0.7018 - val_loss: 0.5480 - val_acc: 0.6983\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5330 - acc: 0.6988 - val_loss: 0.5458 - val_acc: 0.7001\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5388 - acc: 0.7060 - val_loss: 0.5442 - val_acc: 0.7007\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5389 - acc: 0.6944 - val_loss: 0.5430 - val_acc: 0.7020\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5306 - acc: 0.7034 - val_loss: 0.5424 - val_acc: 0.7031\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5297 - acc: 0.7114 - val_loss: 0.5415 - val_acc: 0.7037\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5286 - acc: 0.7082 - val_loss: 0.5404 - val_acc: 0.7043\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5326 - acc: 0.6958 - val_loss: 0.5394 - val_acc: 0.7057\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5269 - acc: 0.7060 - val_loss: 0.5389 - val_acc: 0.7067\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5210 - acc: 0.7222 - val_loss: 0.5393 - val_acc: 0.7073\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5271 - acc: 0.7146 - val_loss: 0.5382 - val_acc: 0.7077\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5223 - acc: 0.7114 - val_loss: 0.5374 - val_acc: 0.7078\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5155 - acc: 0.7200 - val_loss: 0.5364 - val_acc: 0.7089\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5187 - acc: 0.7140 - val_loss: 0.5357 - val_acc: 0.7110\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5188 - acc: 0.7132 - val_loss: 0.5354 - val_acc: 0.7114\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5172 - acc: 0.7250 - val_loss: 0.5333 - val_acc: 0.7124\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5155 - acc: 0.7182 - val_loss: 0.5318 - val_acc: 0.7128\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5135 - acc: 0.7180 - val_loss: 0.5313 - val_acc: 0.7135\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.5076 - acc: 0.7226 - val_loss: 0.5313 - val_acc: 0.7144\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5128 - acc: 0.7252 - val_loss: 0.5320 - val_acc: 0.7150\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5096 - acc: 0.7260 - val_loss: 0.5322 - val_acc: 0.7159\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5030 - acc: 0.7270 - val_loss: 0.5315 - val_acc: 0.7140\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5085 - acc: 0.7284 - val_loss: 0.5313 - val_acc: 0.7148\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5069 - acc: 0.7294 - val_loss: 0.5316 - val_acc: 0.7171\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5073 - acc: 0.7344 - val_loss: 0.5299 - val_acc: 0.7186\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5036 - acc: 0.7310 - val_loss: 0.5273 - val_acc: 0.7192\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5052 - acc: 0.7302 - val_loss: 0.5262 - val_acc: 0.7194\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5019 - acc: 0.7294 - val_loss: 0.5264 - val_acc: 0.7203\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5055 - acc: 0.7286 - val_loss: 0.5264 - val_acc: 0.7200\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4993 - acc: 0.7336 - val_loss: 0.5263 - val_acc: 0.7208\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4991 - acc: 0.7340 - val_loss: 0.5254 - val_acc: 0.7212\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4964 - acc: 0.7360 - val_loss: 0.5249 - val_acc: 0.7220\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4930 - acc: 0.7430 - val_loss: 0.5254 - val_acc: 0.7232\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4977 - acc: 0.7416 - val_loss: 0.5244 - val_acc: 0.7233\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4915 - acc: 0.7430 - val_loss: 0.5241 - val_acc: 0.7232\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4948 - acc: 0.7416 - val_loss: 0.5252 - val_acc: 0.7237\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4930 - acc: 0.7368 - val_loss: 0.5263 - val_acc: 0.7235\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4915 - acc: 0.7448 - val_loss: 0.5226 - val_acc: 0.7239\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4868 - acc: 0.7452 - val_loss: 0.5211 - val_acc: 0.7240\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4901 - acc: 0.7466 - val_loss: 0.5226 - val_acc: 0.7254\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4849 - acc: 0.7470 - val_loss: 0.5240 - val_acc: 0.7254\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4885 - acc: 0.7472 - val_loss: 0.5212 - val_acc: 0.7257\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4914 - acc: 0.7464 - val_loss: 0.5199 - val_acc: 0.7250\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4848 - acc: 0.7458 - val_loss: 0.5208 - val_acc: 0.7267\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4784 - acc: 0.7520 - val_loss: 0.5221 - val_acc: 0.7270\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4814 - acc: 0.7522 - val_loss: 0.5207 - val_acc: 0.7273\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4804 - acc: 0.7558 - val_loss: 0.5182 - val_acc: 0.7275\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4828 - acc: 0.7484 - val_loss: 0.5184 - val_acc: 0.7276\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4829 - acc: 0.7530 - val_loss: 0.5195 - val_acc: 0.7279\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4777 - acc: 0.7600 - val_loss: 0.5178 - val_acc: 0.7284\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4777 - acc: 0.7572 - val_loss: 0.5156 - val_acc: 0.7283\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4787 - acc: 0.7514 - val_loss: 0.5161 - val_acc: 0.7291\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4717 - acc: 0.7574 - val_loss: 0.5187 - val_acc: 0.7288\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4715 - acc: 0.7556 - val_loss: 0.5182 - val_acc: 0.7303\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4792 - acc: 0.7530 - val_loss: 0.5154 - val_acc: 0.7292\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4743 - acc: 0.7562 - val_loss: 0.5136 - val_acc: 0.7314\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4745 - acc: 0.7486 - val_loss: 0.5144 - val_acc: 0.7300\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4691 - acc: 0.7632 - val_loss: 0.5141 - val_acc: 0.7298\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4654 - acc: 0.7602 - val_loss: 0.5125 - val_acc: 0.7306\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4663 - acc: 0.7592 - val_loss: 0.5148 - val_acc: 0.7303\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4643 - acc: 0.7696 - val_loss: 0.5180 - val_acc: 0.7309\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4619 - acc: 0.7684 - val_loss: 0.5189 - val_acc: 0.7304\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4617 - acc: 0.7668 - val_loss: 0.5188 - val_acc: 0.7305\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4655 - acc: 0.7610 - val_loss: 0.5171 - val_acc: 0.7306\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4620 - acc: 0.7602 - val_loss: 0.5158 - val_acc: 0.7315\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4645 - acc: 0.7568 - val_loss: 0.5144 - val_acc: 0.7320\n",
      "\n",
      " \n",
      "\n",
      "Training model with 10000 training points:\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6862 - acc: 0.6406 - val_loss: 0.6672 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6644 - acc: 0.6684 - val_loss: 0.6475 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6529 - acc: 0.6684 - val_loss: 0.6325 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6444 - acc: 0.6684 - val_loss: 0.6207 - val_acc: 0.6604\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6290 - acc: 0.6684 - val_loss: 0.6122 - val_acc: 0.6604\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6166 - acc: 0.6684 - val_loss: 0.6043 - val_acc: 0.6604\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6074 - acc: 0.6684 - val_loss: 0.5973 - val_acc: 0.6604\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5984 - acc: 0.6684 - val_loss: 0.5934 - val_acc: 0.6604\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5937 - acc: 0.6684 - val_loss: 0.5896 - val_acc: 0.6604\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5891 - acc: 0.6684 - val_loss: 0.5841 - val_acc: 0.6604\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5842 - acc: 0.6684 - val_loss: 0.5794 - val_acc: 0.6607\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5821 - acc: 0.6688 - val_loss: 0.5762 - val_acc: 0.6615\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5764 - acc: 0.6698 - val_loss: 0.5747 - val_acc: 0.6621\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5761 - acc: 0.6697 - val_loss: 0.5743 - val_acc: 0.6628\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5756 - acc: 0.6719 - val_loss: 0.5725 - val_acc: 0.6634\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.5703 - acc: 0.6696 - val_loss: 0.5701 - val_acc: 0.6641\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5704 - acc: 0.6727 - val_loss: 0.5677 - val_acc: 0.6644\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5677 - acc: 0.6747 - val_loss: 0.5659 - val_acc: 0.6659\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5642 - acc: 0.6720 - val_loss: 0.5650 - val_acc: 0.6683\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5615 - acc: 0.6769 - val_loss: 0.5642 - val_acc: 0.6723\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5625 - acc: 0.6801 - val_loss: 0.5619 - val_acc: 0.6764\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5592 - acc: 0.6820 - val_loss: 0.5602 - val_acc: 0.6761\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5569 - acc: 0.6807 - val_loss: 0.5595 - val_acc: 0.6755\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5550 - acc: 0.6838 - val_loss: 0.5597 - val_acc: 0.6753\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5556 - acc: 0.6841 - val_loss: 0.5591 - val_acc: 0.6761\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5534 - acc: 0.6843 - val_loss: 0.5575 - val_acc: 0.6783\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5523 - acc: 0.6850 - val_loss: 0.5556 - val_acc: 0.6813\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5493 - acc: 0.6920 - val_loss: 0.5542 - val_acc: 0.6838\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5464 - acc: 0.6904 - val_loss: 0.5531 - val_acc: 0.6864\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5465 - acc: 0.6907 - val_loss: 0.5513 - val_acc: 0.6883\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5475 - acc: 0.6940 - val_loss: 0.5490 - val_acc: 0.6908\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5437 - acc: 0.6971 - val_loss: 0.5470 - val_acc: 0.6935\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5423 - acc: 0.7012 - val_loss: 0.5455 - val_acc: 0.6953\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5408 - acc: 0.6996 - val_loss: 0.5446 - val_acc: 0.6968\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5399 - acc: 0.7007 - val_loss: 0.5431 - val_acc: 0.6977\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5349 - acc: 0.7017 - val_loss: 0.5417 - val_acc: 0.6987\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5358 - acc: 0.7084 - val_loss: 0.5409 - val_acc: 0.6988\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5355 - acc: 0.7091 - val_loss: 0.5401 - val_acc: 0.7003\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5361 - acc: 0.7054 - val_loss: 0.5391 - val_acc: 0.7007\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5363 - acc: 0.7028 - val_loss: 0.5378 - val_acc: 0.7020\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5297 - acc: 0.7106 - val_loss: 0.5364 - val_acc: 0.7033\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5298 - acc: 0.7131 - val_loss: 0.5351 - val_acc: 0.7051\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5301 - acc: 0.7102 - val_loss: 0.5341 - val_acc: 0.7066\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5264 - acc: 0.7129 - val_loss: 0.5330 - val_acc: 0.7075\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5248 - acc: 0.7150 - val_loss: 0.5316 - val_acc: 0.7087\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5242 - acc: 0.7194 - val_loss: 0.5308 - val_acc: 0.7091\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5225 - acc: 0.7173 - val_loss: 0.5298 - val_acc: 0.7102\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5206 - acc: 0.7184 - val_loss: 0.5281 - val_acc: 0.7113\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5207 - acc: 0.7209 - val_loss: 0.5264 - val_acc: 0.7134\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5183 - acc: 0.7206 - val_loss: 0.5252 - val_acc: 0.7146\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5196 - acc: 0.7201 - val_loss: 0.5242 - val_acc: 0.7154\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5167 - acc: 0.7194 - val_loss: 0.5231 - val_acc: 0.7165\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5151 - acc: 0.7234 - val_loss: 0.5219 - val_acc: 0.7186\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5134 - acc: 0.7251 - val_loss: 0.5208 - val_acc: 0.7192\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5132 - acc: 0.7235 - val_loss: 0.5207 - val_acc: 0.7201\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5150 - acc: 0.7234 - val_loss: 0.5200 - val_acc: 0.7203\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5103 - acc: 0.7287 - val_loss: 0.5185 - val_acc: 0.7214\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5100 - acc: 0.7301 - val_loss: 0.5178 - val_acc: 0.7217\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5109 - acc: 0.7268 - val_loss: 0.5176 - val_acc: 0.7220\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5083 - acc: 0.7330 - val_loss: 0.5157 - val_acc: 0.7233\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5031 - acc: 0.7345 - val_loss: 0.5144 - val_acc: 0.7238\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5060 - acc: 0.7368 - val_loss: 0.5144 - val_acc: 0.7245\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5034 - acc: 0.7338 - val_loss: 0.5145 - val_acc: 0.7251\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5002 - acc: 0.7352 - val_loss: 0.5123 - val_acc: 0.7271\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5011 - acc: 0.7391 - val_loss: 0.5106 - val_acc: 0.7284\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4999 - acc: 0.7327 - val_loss: 0.5106 - val_acc: 0.7289\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4987 - acc: 0.7403 - val_loss: 0.5093 - val_acc: 0.7295\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4966 - acc: 0.7387 - val_loss: 0.5078 - val_acc: 0.7301\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4969 - acc: 0.7390 - val_loss: 0.5074 - val_acc: 0.7307\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4985 - acc: 0.7381 - val_loss: 0.5087 - val_acc: 0.7295\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4982 - acc: 0.7394 - val_loss: 0.5067 - val_acc: 0.7306\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4946 - acc: 0.7360 - val_loss: 0.5046 - val_acc: 0.7318\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4944 - acc: 0.7348 - val_loss: 0.5041 - val_acc: 0.7320\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4947 - acc: 0.7419 - val_loss: 0.5063 - val_acc: 0.7312\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4934 - acc: 0.7440 - val_loss: 0.5040 - val_acc: 0.7328\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4911 - acc: 0.7421 - val_loss: 0.5016 - val_acc: 0.7351\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4941 - acc: 0.7356 - val_loss: 0.5015 - val_acc: 0.7350\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4919 - acc: 0.7416 - val_loss: 0.5023 - val_acc: 0.7339\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.4859 - acc: 0.7460 - val_loss: 0.5012 - val_acc: 0.7348\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4885 - acc: 0.7443 - val_loss: 0.4983 - val_acc: 0.7378\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4869 - acc: 0.7462 - val_loss: 0.4979 - val_acc: 0.7382\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4850 - acc: 0.7505 - val_loss: 0.5002 - val_acc: 0.7371\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4839 - acc: 0.7540 - val_loss: 0.5003 - val_acc: 0.7375\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4874 - acc: 0.7505 - val_loss: 0.4962 - val_acc: 0.7396\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4811 - acc: 0.7532 - val_loss: 0.4954 - val_acc: 0.7402\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4815 - acc: 0.7495 - val_loss: 0.4968 - val_acc: 0.7386\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4786 - acc: 0.7480 - val_loss: 0.4970 - val_acc: 0.7387\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4760 - acc: 0.7608 - val_loss: 0.4950 - val_acc: 0.7408\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4793 - acc: 0.7498 - val_loss: 0.4943 - val_acc: 0.7415\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4807 - acc: 0.7461 - val_loss: 0.4944 - val_acc: 0.7414\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4766 - acc: 0.7570 - val_loss: 0.4944 - val_acc: 0.7409\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4757 - acc: 0.7569 - val_loss: 0.4923 - val_acc: 0.7414\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4725 - acc: 0.7555 - val_loss: 0.4913 - val_acc: 0.7422\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4788 - acc: 0.7530 - val_loss: 0.4916 - val_acc: 0.7425\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4721 - acc: 0.7585 - val_loss: 0.4913 - val_acc: 0.7435\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4698 - acc: 0.7619 - val_loss: 0.4904 - val_acc: 0.7445\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4713 - acc: 0.7581 - val_loss: 0.4890 - val_acc: 0.7455\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4677 - acc: 0.7554 - val_loss: 0.4885 - val_acc: 0.7458\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4750 - acc: 0.7547 - val_loss: 0.4879 - val_acc: 0.7457\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4686 - acc: 0.7609 - val_loss: 0.4869 - val_acc: 0.7466\n",
      "\n",
      " \n",
      "\n",
      "Training model with 12500 training points:\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 4s 887ms/step - loss: 0.6878 - acc: 0.5166 - val_loss: 0.6412 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.6525 - acc: 0.6651 - val_loss: 0.6112 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.6133 - acc: 0.6637 - val_loss: 0.5925 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.5973 - acc: 0.6634 - val_loss: 0.5804 - val_acc: 0.6611\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 3s 759ms/step - loss: 0.5838 - acc: 0.6682 - val_loss: 0.5771 - val_acc: 0.6699\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.5823 - acc: 0.6702 - val_loss: 0.5737 - val_acc: 0.6708\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.5739 - acc: 0.6753 - val_loss: 0.5670 - val_acc: 0.6728\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.5657 - acc: 0.6803 - val_loss: 0.5620 - val_acc: 0.6757\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.5633 - acc: 0.6819 - val_loss: 0.5597 - val_acc: 0.6777\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 3s 748ms/step - loss: 0.5614 - acc: 0.6837 - val_loss: 0.5571 - val_acc: 0.6804\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 2s 657ms/step - loss: 0.5584 - acc: 0.6873 - val_loss: 0.5546 - val_acc: 0.6834\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 2s 666ms/step - loss: 0.5557 - acc: 0.6859 - val_loss: 0.5536 - val_acc: 0.6853\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 2s 704ms/step - loss: 0.5511 - acc: 0.6875 - val_loss: 0.5493 - val_acc: 0.6903\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 2s 623ms/step - loss: 0.5491 - acc: 0.6899 - val_loss: 0.5495 - val_acc: 0.6926\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.5451 - acc: 0.6983 - val_loss: 0.5464 - val_acc: 0.6921\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.5415 - acc: 0.6991 - val_loss: 0.5411 - val_acc: 0.6999\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.5382 - acc: 0.7044 - val_loss: 0.5404 - val_acc: 0.7029\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.5389 - acc: 0.7061 - val_loss: 0.5410 - val_acc: 0.7020\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.5432 - acc: 0.7003 - val_loss: 0.5371 - val_acc: 0.7033\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.5335 - acc: 0.7110 - val_loss: 0.5343 - val_acc: 0.7063\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.5305 - acc: 0.7088 - val_loss: 0.5345 - val_acc: 0.7061\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 3s 773ms/step - loss: 0.5319 - acc: 0.7118 - val_loss: 0.5332 - val_acc: 0.7070\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 3s 771ms/step - loss: 0.5302 - acc: 0.7145 - val_loss: 0.5303 - val_acc: 0.7080\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.5283 - acc: 0.7131 - val_loss: 0.5271 - val_acc: 0.7127\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.5249 - acc: 0.7194 - val_loss: 0.5230 - val_acc: 0.7167\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.5221 - acc: 0.7212 - val_loss: 0.5210 - val_acc: 0.7173\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.5190 - acc: 0.7172 - val_loss: 0.5240 - val_acc: 0.7197\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.5203 - acc: 0.7243 - val_loss: 0.5203 - val_acc: 0.7170\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.5199 - acc: 0.7189 - val_loss: 0.5177 - val_acc: 0.7250\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.5183 - acc: 0.7309 - val_loss: 0.5133 - val_acc: 0.7274\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.5214 - acc: 0.7234 - val_loss: 0.5150 - val_acc: 0.7225\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.5117 - acc: 0.7241 - val_loss: 0.5208 - val_acc: 0.7166\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 3s 756ms/step - loss: 0.5130 - acc: 0.7230 - val_loss: 0.5150 - val_acc: 0.7219\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.5187 - acc: 0.7199 - val_loss: 0.5094 - val_acc: 0.7303\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.5111 - acc: 0.7337 - val_loss: 0.5115 - val_acc: 0.7287\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.5055 - acc: 0.7358 - val_loss: 0.5081 - val_acc: 0.7313\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.5014 - acc: 0.7363 - val_loss: 0.5071 - val_acc: 0.7319\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.5050 - acc: 0.7349 - val_loss: 0.5052 - val_acc: 0.7347\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.4985 - acc: 0.7373 - val_loss: 0.5051 - val_acc: 0.7345\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.5006 - acc: 0.7372 - val_loss: 0.5036 - val_acc: 0.7343\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4992 - acc: 0.7391 - val_loss: 0.5002 - val_acc: 0.7373\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.4986 - acc: 0.7375 - val_loss: 0.4990 - val_acc: 0.7358\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.4942 - acc: 0.7414 - val_loss: 0.4968 - val_acc: 0.7381\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.4971 - acc: 0.7375 - val_loss: 0.4954 - val_acc: 0.7426\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.4905 - acc: 0.7456 - val_loss: 0.4964 - val_acc: 0.7397\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.4972 - acc: 0.7404 - val_loss: 0.4951 - val_acc: 0.7416\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 3s 800ms/step - loss: 0.4909 - acc: 0.7447 - val_loss: 0.4974 - val_acc: 0.7424\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.4967 - acc: 0.7403 - val_loss: 0.4943 - val_acc: 0.7444\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.4955 - acc: 0.7368 - val_loss: 0.4939 - val_acc: 0.7419\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4903 - acc: 0.7465 - val_loss: 0.4920 - val_acc: 0.7425\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4904 - acc: 0.7393 - val_loss: 0.4892 - val_acc: 0.7444\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.4826 - acc: 0.7489 - val_loss: 0.4896 - val_acc: 0.7447\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.4813 - acc: 0.7507 - val_loss: 0.4874 - val_acc: 0.7473\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 3s 761ms/step - loss: 0.4783 - acc: 0.7511 - val_loss: 0.4853 - val_acc: 0.7467\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.4797 - acc: 0.7509 - val_loss: 0.4845 - val_acc: 0.7463\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.4849 - acc: 0.7449 - val_loss: 0.4848 - val_acc: 0.7469\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.4776 - acc: 0.7559 - val_loss: 0.4867 - val_acc: 0.7481\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4825 - acc: 0.7508 - val_loss: 0.4878 - val_acc: 0.7494\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.4778 - acc: 0.7525 - val_loss: 0.4864 - val_acc: 0.7491\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 3s 753ms/step - loss: 0.4745 - acc: 0.7512 - val_loss: 0.4829 - val_acc: 0.7470\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 3s 760ms/step - loss: 0.4797 - acc: 0.7464 - val_loss: 0.4824 - val_acc: 0.7488\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.4752 - acc: 0.7518 - val_loss: 0.4807 - val_acc: 0.7508\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4722 - acc: 0.7541 - val_loss: 0.4818 - val_acc: 0.7510\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4740 - acc: 0.7526 - val_loss: 0.4808 - val_acc: 0.7510\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.4762 - acc: 0.7505 - val_loss: 0.4783 - val_acc: 0.7504\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 3s 762ms/step - loss: 0.4680 - acc: 0.7607 - val_loss: 0.4804 - val_acc: 0.7521\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.4711 - acc: 0.7565 - val_loss: 0.4751 - val_acc: 0.7513\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4709 - acc: 0.7557 - val_loss: 0.4735 - val_acc: 0.7538\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.4668 - acc: 0.7590 - val_loss: 0.4765 - val_acc: 0.7524\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 3s 776ms/step - loss: 0.4676 - acc: 0.7592 - val_loss: 0.4734 - val_acc: 0.7554\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.4701 - acc: 0.7556 - val_loss: 0.4808 - val_acc: 0.7532\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4676 - acc: 0.7573 - val_loss: 0.4711 - val_acc: 0.7566\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 3s 775ms/step - loss: 0.4658 - acc: 0.7589 - val_loss: 0.4707 - val_acc: 0.7546\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.4630 - acc: 0.7663 - val_loss: 0.4788 - val_acc: 0.7540\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4622 - acc: 0.7626 - val_loss: 0.4719 - val_acc: 0.7553\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.4625 - acc: 0.7652 - val_loss: 0.4727 - val_acc: 0.7529\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 3s 774ms/step - loss: 0.4646 - acc: 0.7591 - val_loss: 0.4702 - val_acc: 0.7569\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.4613 - acc: 0.7590 - val_loss: 0.4721 - val_acc: 0.7570\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.4600 - acc: 0.7622 - val_loss: 0.4702 - val_acc: 0.7564\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 3s 763ms/step - loss: 0.4536 - acc: 0.7676 - val_loss: 0.4701 - val_acc: 0.7565\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 3s 765ms/step - loss: 0.4596 - acc: 0.7576 - val_loss: 0.4709 - val_acc: 0.7584\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 3s 769ms/step - loss: 0.4569 - acc: 0.7684 - val_loss: 0.4667 - val_acc: 0.7590\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4574 - acc: 0.7609 - val_loss: 0.4667 - val_acc: 0.7583\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 3s 755ms/step - loss: 0.4539 - acc: 0.7669 - val_loss: 0.4702 - val_acc: 0.7589\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.4580 - acc: 0.7656 - val_loss: 0.4668 - val_acc: 0.7597\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 3s 757ms/step - loss: 0.4521 - acc: 0.7651 - val_loss: 0.4684 - val_acc: 0.7558\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 3s 758ms/step - loss: 0.4542 - acc: 0.7663 - val_loss: 0.4666 - val_acc: 0.7592\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 3s 766ms/step - loss: 0.4517 - acc: 0.7685 - val_loss: 0.4703 - val_acc: 0.7596\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.4549 - acc: 0.7692 - val_loss: 0.4695 - val_acc: 0.7586\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 3s 768ms/step - loss: 0.4516 - acc: 0.7695 - val_loss: 0.4623 - val_acc: 0.7630\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 0.4499 - acc: 0.7731 - val_loss: 0.4636 - val_acc: 0.7615\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 3s 770ms/step - loss: 0.4484 - acc: 0.7709 - val_loss: 0.4653 - val_acc: 0.7598\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 3s 797ms/step - loss: 0.4501 - acc: 0.7686 - val_loss: 0.4659 - val_acc: 0.7620\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 3s 767ms/step - loss: 0.4489 - acc: 0.7715 - val_loss: 0.4614 - val_acc: 0.7623\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 3s 754ms/step - loss: 0.4507 - acc: 0.7672 - val_loss: 0.4597 - val_acc: 0.7629\n",
      "\n",
      " \n",
      "\n",
      "Training model with 15000 training points:\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 5s 935ms/step - loss: 0.6879 - acc: 0.5215 - val_loss: 0.6439 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.6473 - acc: 0.6670 - val_loss: 0.6051 - val_acc: 0.6604\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 3s 823ms/step - loss: 0.6116 - acc: 0.6621 - val_loss: 0.5875 - val_acc: 0.6604\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 3s 838ms/step - loss: 0.5914 - acc: 0.6621 - val_loss: 0.5774 - val_acc: 0.6637\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 3s 828ms/step - loss: 0.5803 - acc: 0.6677 - val_loss: 0.5676 - val_acc: 0.6660\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.5721 - acc: 0.6696 - val_loss: 0.5626 - val_acc: 0.6690\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5641 - acc: 0.6774 - val_loss: 0.5578 - val_acc: 0.6781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.5591 - acc: 0.6825 - val_loss: 0.5550 - val_acc: 0.6774\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 3s 821ms/step - loss: 0.5521 - acc: 0.6868 - val_loss: 0.5476 - val_acc: 0.6878\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.5497 - acc: 0.6913 - val_loss: 0.5438 - val_acc: 0.6928\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5391 - acc: 0.7067 - val_loss: 0.5396 - val_acc: 0.6965\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5451 - acc: 0.6979 - val_loss: 0.5377 - val_acc: 0.7037\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5347 - acc: 0.7093 - val_loss: 0.5316 - val_acc: 0.7092\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 3s 826ms/step - loss: 0.5313 - acc: 0.7097 - val_loss: 0.5279 - val_acc: 0.7114\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 3s 841ms/step - loss: 0.5283 - acc: 0.7111 - val_loss: 0.5237 - val_acc: 0.7155\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.5265 - acc: 0.7166 - val_loss: 0.5214 - val_acc: 0.7186\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.5208 - acc: 0.7224 - val_loss: 0.5190 - val_acc: 0.7201\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.5163 - acc: 0.7232 - val_loss: 0.5171 - val_acc: 0.7237\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 3s 825ms/step - loss: 0.5107 - acc: 0.7288 - val_loss: 0.5135 - val_acc: 0.7245\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5107 - acc: 0.7261 - val_loss: 0.5111 - val_acc: 0.7272\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 0.5102 - acc: 0.7292 - val_loss: 0.5067 - val_acc: 0.7309\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 3s 821ms/step - loss: 0.5053 - acc: 0.7317 - val_loss: 0.5042 - val_acc: 0.7332\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 3s 838ms/step - loss: 0.4998 - acc: 0.7387 - val_loss: 0.5001 - val_acc: 0.7369\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.5002 - acc: 0.7396 - val_loss: 0.4973 - val_acc: 0.7402\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4953 - acc: 0.7430 - val_loss: 0.4977 - val_acc: 0.7396\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.4952 - acc: 0.7425 - val_loss: 0.4944 - val_acc: 0.7414\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.4921 - acc: 0.7422 - val_loss: 0.4915 - val_acc: 0.7439\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.4868 - acc: 0.7460 - val_loss: 0.4914 - val_acc: 0.7446\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 3s 822ms/step - loss: 0.4833 - acc: 0.7495 - val_loss: 0.4892 - val_acc: 0.7458\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.4827 - acc: 0.7514 - val_loss: 0.4873 - val_acc: 0.7474\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.4799 - acc: 0.7530 - val_loss: 0.4859 - val_acc: 0.7460\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4744 - acc: 0.7550 - val_loss: 0.4828 - val_acc: 0.7488\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 3s 838ms/step - loss: 0.4803 - acc: 0.7501 - val_loss: 0.4854 - val_acc: 0.7478\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4778 - acc: 0.7533 - val_loss: 0.4832 - val_acc: 0.7512\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4697 - acc: 0.7556 - val_loss: 0.4788 - val_acc: 0.7515\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.4703 - acc: 0.7590 - val_loss: 0.4764 - val_acc: 0.7537\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 3s 838ms/step - loss: 0.4640 - acc: 0.7630 - val_loss: 0.4777 - val_acc: 0.7532\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4691 - acc: 0.7603 - val_loss: 0.4733 - val_acc: 0.7549\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4648 - acc: 0.7585 - val_loss: 0.4756 - val_acc: 0.7543\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 3s 861ms/step - loss: 0.4642 - acc: 0.7610 - val_loss: 0.4736 - val_acc: 0.7547\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 3s 825ms/step - loss: 0.4600 - acc: 0.7651 - val_loss: 0.4682 - val_acc: 0.7563\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.4623 - acc: 0.7659 - val_loss: 0.4673 - val_acc: 0.7578\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 3s 818ms/step - loss: 0.4588 - acc: 0.7646 - val_loss: 0.4690 - val_acc: 0.7572\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.4524 - acc: 0.7708 - val_loss: 0.4647 - val_acc: 0.7598\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4543 - acc: 0.7696 - val_loss: 0.4652 - val_acc: 0.7607\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 3s 839ms/step - loss: 0.4514 - acc: 0.7718 - val_loss: 0.4641 - val_acc: 0.7616\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 3s 828ms/step - loss: 0.4559 - acc: 0.7643 - val_loss: 0.4604 - val_acc: 0.7635\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 3s 820ms/step - loss: 0.4503 - acc: 0.7705 - val_loss: 0.4602 - val_acc: 0.7625\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 3s 823ms/step - loss: 0.4480 - acc: 0.7747 - val_loss: 0.4599 - val_acc: 0.7630\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 3s 830ms/step - loss: 0.4466 - acc: 0.7712 - val_loss: 0.4574 - val_acc: 0.7652\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.4470 - acc: 0.7717 - val_loss: 0.4573 - val_acc: 0.7650\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.4424 - acc: 0.7732 - val_loss: 0.4568 - val_acc: 0.7659\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.4377 - acc: 0.7772 - val_loss: 0.4566 - val_acc: 0.7652\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4428 - acc: 0.7779 - val_loss: 0.4540 - val_acc: 0.7661\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4417 - acc: 0.7766 - val_loss: 0.4583 - val_acc: 0.7662\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 3s 830ms/step - loss: 0.4431 - acc: 0.7734 - val_loss: 0.4529 - val_acc: 0.7678\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4331 - acc: 0.7834 - val_loss: 0.4520 - val_acc: 0.7689\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.4346 - acc: 0.7785 - val_loss: 0.4529 - val_acc: 0.7692\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4298 - acc: 0.7863 - val_loss: 0.4509 - val_acc: 0.7697\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.4275 - acc: 0.7828 - val_loss: 0.4489 - val_acc: 0.7693\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.4317 - acc: 0.7850 - val_loss: 0.4503 - val_acc: 0.7694\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.4293 - acc: 0.7866 - val_loss: 0.4500 - val_acc: 0.7709\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 3s 812ms/step - loss: 0.4267 - acc: 0.7885 - val_loss: 0.4476 - val_acc: 0.7714\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 3s 751ms/step - loss: 0.4241 - acc: 0.7869 - val_loss: 0.4488 - val_acc: 0.7701\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 2s 679ms/step - loss: 0.4244 - acc: 0.7870 - val_loss: 0.4488 - val_acc: 0.7723\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.4215 - acc: 0.7885 - val_loss: 0.4460 - val_acc: 0.7729\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 3s 837ms/step - loss: 0.4217 - acc: 0.7876 - val_loss: 0.4450 - val_acc: 0.7730\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.4186 - acc: 0.7881 - val_loss: 0.4484 - val_acc: 0.7735\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 3s 844ms/step - loss: 0.4217 - acc: 0.7857 - val_loss: 0.4468 - val_acc: 0.7744\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4174 - acc: 0.7930 - val_loss: 0.4435 - val_acc: 0.7752\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 3s 820ms/step - loss: 0.4173 - acc: 0.7883 - val_loss: 0.4462 - val_acc: 0.7745\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4151 - acc: 0.7935 - val_loss: 0.4462 - val_acc: 0.7744\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 0.4110 - acc: 0.7947 - val_loss: 0.4432 - val_acc: 0.7769\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.4123 - acc: 0.7938 - val_loss: 0.4416 - val_acc: 0.7765\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 3s 830ms/step - loss: 0.4110 - acc: 0.7941 - val_loss: 0.4422 - val_acc: 0.7776\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4076 - acc: 0.7981 - val_loss: 0.4444 - val_acc: 0.7773\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.4057 - acc: 0.8011 - val_loss: 0.4399 - val_acc: 0.7778\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.4054 - acc: 0.7987 - val_loss: 0.4417 - val_acc: 0.7790\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.4008 - acc: 0.7996 - val_loss: 0.4399 - val_acc: 0.7799\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.4055 - acc: 0.7983 - val_loss: 0.4412 - val_acc: 0.7797\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.3975 - acc: 0.8068 - val_loss: 0.4401 - val_acc: 0.7785\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.3969 - acc: 0.8068 - val_loss: 0.4382 - val_acc: 0.7808\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 3s 864ms/step - loss: 0.3970 - acc: 0.8098 - val_loss: 0.4394 - val_acc: 0.7811\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.3980 - acc: 0.8018 - val_loss: 0.4373 - val_acc: 0.7820\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 3s 845ms/step - loss: 0.3964 - acc: 0.8050 - val_loss: 0.4389 - val_acc: 0.7785\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.3957 - acc: 0.8012 - val_loss: 0.4415 - val_acc: 0.7821\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.3975 - acc: 0.8031 - val_loss: 0.4369 - val_acc: 0.7831\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 3s 834ms/step - loss: 0.3912 - acc: 0.8104 - val_loss: 0.4394 - val_acc: 0.7824\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.3929 - acc: 0.8101 - val_loss: 0.4385 - val_acc: 0.7820\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.3904 - acc: 0.8056 - val_loss: 0.4374 - val_acc: 0.7845\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 3s 840ms/step - loss: 0.3835 - acc: 0.8127 - val_loss: 0.4386 - val_acc: 0.7832\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 3s 833ms/step - loss: 0.3865 - acc: 0.8136 - val_loss: 0.4366 - val_acc: 0.7854\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 3s 831ms/step - loss: 0.3866 - acc: 0.8092 - val_loss: 0.4363 - val_acc: 0.7851\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.3761 - acc: 0.8194 - val_loss: 0.4396 - val_acc: 0.7860\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.3795 - acc: 0.8128 - val_loss: 0.4377 - val_acc: 0.7868\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 3s 829ms/step - loss: 0.3821 - acc: 0.8130 - val_loss: 0.4339 - val_acc: 0.7870\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 3s 832ms/step - loss: 0.3760 - acc: 0.8171 - val_loss: 0.4345 - val_acc: 0.7857\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 3s 836ms/step - loss: 0.3737 - acc: 0.8162 - val_loss: 0.4321 - val_acc: 0.7867\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 0.3749 - acc: 0.8201 - val_loss: 0.4364 - val_acc: 0.7873\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 3s 826ms/step - loss: 0.3764 - acc: 0.8147 - val_loss: 0.4340 - val_acc: 0.7862\n",
      "\n",
      " \n",
      "\n",
      "Training model with 20000 training points:\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 5s 803ms/step - loss: 0.6786 - acc: 0.5889 - val_loss: 0.6249 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.6196 - acc: 0.6669 - val_loss: 0.5851 - val_acc: 0.6609\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.5889 - acc: 0.6651 - val_loss: 0.5812 - val_acc: 0.6694\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.5788 - acc: 0.6710 - val_loss: 0.5656 - val_acc: 0.6697\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.5670 - acc: 0.6706 - val_loss: 0.5594 - val_acc: 0.6737\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.5586 - acc: 0.6807 - val_loss: 0.5553 - val_acc: 0.6766\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5528 - acc: 0.6864 - val_loss: 0.5478 - val_acc: 0.6881\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.5463 - acc: 0.6911 - val_loss: 0.5401 - val_acc: 0.6982\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.5396 - acc: 0.6965 - val_loss: 0.5361 - val_acc: 0.7034\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.5348 - acc: 0.7071 - val_loss: 0.5280 - val_acc: 0.7100\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.5280 - acc: 0.7120 - val_loss: 0.5238 - val_acc: 0.7152\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.5223 - acc: 0.7164 - val_loss: 0.5179 - val_acc: 0.7203\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.5212 - acc: 0.7155 - val_loss: 0.5161 - val_acc: 0.7221\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.5176 - acc: 0.7194 - val_loss: 0.5117 - val_acc: 0.7255\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.5102 - acc: 0.7241 - val_loss: 0.5056 - val_acc: 0.7307\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.5047 - acc: 0.7313 - val_loss: 0.5045 - val_acc: 0.7313\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.5021 - acc: 0.7324 - val_loss: 0.5010 - val_acc: 0.7345\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5045 - acc: 0.7352 - val_loss: 0.5008 - val_acc: 0.7354\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.4987 - acc: 0.7361 - val_loss: 0.4962 - val_acc: 0.7387\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.4937 - acc: 0.7405 - val_loss: 0.4904 - val_acc: 0.7429\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 3s 698ms/step - loss: 0.4908 - acc: 0.7383 - val_loss: 0.4898 - val_acc: 0.7444\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 3s 735ms/step - loss: 0.4867 - acc: 0.7431 - val_loss: 0.4856 - val_acc: 0.7470\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.4876 - acc: 0.7422 - val_loss: 0.4838 - val_acc: 0.7478\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.4836 - acc: 0.7470 - val_loss: 0.4847 - val_acc: 0.7480\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.4858 - acc: 0.7414 - val_loss: 0.4794 - val_acc: 0.7510\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.4770 - acc: 0.7510 - val_loss: 0.4785 - val_acc: 0.7515\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.4752 - acc: 0.7540 - val_loss: 0.4754 - val_acc: 0.7536\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.4746 - acc: 0.7528 - val_loss: 0.4730 - val_acc: 0.7552\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.4682 - acc: 0.7538 - val_loss: 0.4703 - val_acc: 0.7558\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.4735 - acc: 0.7493 - val_loss: 0.4700 - val_acc: 0.7553\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 3s 730ms/step - loss: 0.4641 - acc: 0.7520 - val_loss: 0.4685 - val_acc: 0.7582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.4652 - acc: 0.7612 - val_loss: 0.4662 - val_acc: 0.7575\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 3s 728ms/step - loss: 0.4649 - acc: 0.7536 - val_loss: 0.4651 - val_acc: 0.7582\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4612 - acc: 0.7623 - val_loss: 0.4642 - val_acc: 0.7581\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4579 - acc: 0.7604 - val_loss: 0.4608 - val_acc: 0.7594\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 3s 709ms/step - loss: 0.4565 - acc: 0.7625 - val_loss: 0.4613 - val_acc: 0.7605\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.4572 - acc: 0.7628 - val_loss: 0.4595 - val_acc: 0.7632\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.4532 - acc: 0.7646 - val_loss: 0.4607 - val_acc: 0.7613\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.4536 - acc: 0.7657 - val_loss: 0.4549 - val_acc: 0.7647\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.4472 - acc: 0.7688 - val_loss: 0.4561 - val_acc: 0.7632\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4455 - acc: 0.7683 - val_loss: 0.4528 - val_acc: 0.7675\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.4406 - acc: 0.7708 - val_loss: 0.4546 - val_acc: 0.7647\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4458 - acc: 0.7670 - val_loss: 0.4519 - val_acc: 0.7685\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4421 - acc: 0.7729 - val_loss: 0.4502 - val_acc: 0.7674\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.4451 - acc: 0.7714 - val_loss: 0.4495 - val_acc: 0.7698\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 3s 713ms/step - loss: 0.4392 - acc: 0.7737 - val_loss: 0.4510 - val_acc: 0.7673\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.4412 - acc: 0.7753 - val_loss: 0.4462 - val_acc: 0.7718\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.4358 - acc: 0.7727 - val_loss: 0.4478 - val_acc: 0.7709\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.4345 - acc: 0.7736 - val_loss: 0.4438 - val_acc: 0.7737\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.4285 - acc: 0.7804 - val_loss: 0.4468 - val_acc: 0.7689\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.4342 - acc: 0.7754 - val_loss: 0.4422 - val_acc: 0.7747\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 3s 734ms/step - loss: 0.4318 - acc: 0.7783 - val_loss: 0.4497 - val_acc: 0.7723\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.4320 - acc: 0.7810 - val_loss: 0.4430 - val_acc: 0.7721\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.4274 - acc: 0.7817 - val_loss: 0.4403 - val_acc: 0.7754\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 3s 728ms/step - loss: 0.4254 - acc: 0.7847 - val_loss: 0.4413 - val_acc: 0.7767\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.4193 - acc: 0.7881 - val_loss: 0.4362 - val_acc: 0.7782\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.4218 - acc: 0.7835 - val_loss: 0.4395 - val_acc: 0.7786\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.4197 - acc: 0.7908 - val_loss: 0.4373 - val_acc: 0.7795\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 3s 736ms/step - loss: 0.4224 - acc: 0.7821 - val_loss: 0.4354 - val_acc: 0.7816\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 3s 729ms/step - loss: 0.4170 - acc: 0.7852 - val_loss: 0.4336 - val_acc: 0.7821\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.4148 - acc: 0.7905 - val_loss: 0.4315 - val_acc: 0.7830\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.4139 - acc: 0.7885 - val_loss: 0.4316 - val_acc: 0.7827\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.4107 - acc: 0.7960 - val_loss: 0.4323 - val_acc: 0.7820\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.4151 - acc: 0.7911 - val_loss: 0.4281 - val_acc: 0.7854\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.4095 - acc: 0.7944 - val_loss: 0.4306 - val_acc: 0.7846\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 3s 731ms/step - loss: 0.4068 - acc: 0.7981 - val_loss: 0.4311 - val_acc: 0.7811\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.4120 - acc: 0.7893 - val_loss: 0.4283 - val_acc: 0.7856\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.4061 - acc: 0.7951 - val_loss: 0.4296 - val_acc: 0.7853\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.4087 - acc: 0.7938 - val_loss: 0.4306 - val_acc: 0.7861\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.4068 - acc: 0.8008 - val_loss: 0.4280 - val_acc: 0.7864\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.3989 - acc: 0.8012 - val_loss: 0.4276 - val_acc: 0.7869\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 3s 708ms/step - loss: 0.3966 - acc: 0.8002 - val_loss: 0.4254 - val_acc: 0.7877\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 3s 709ms/step - loss: 0.4000 - acc: 0.8011 - val_loss: 0.4242 - val_acc: 0.7921\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 3s 702ms/step - loss: 0.3937 - acc: 0.8008 - val_loss: 0.4218 - val_acc: 0.7915\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 3s 703ms/step - loss: 0.3924 - acc: 0.8071 - val_loss: 0.4246 - val_acc: 0.7891\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 3s 711ms/step - loss: 0.3884 - acc: 0.8063 - val_loss: 0.4223 - val_acc: 0.7927\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.3900 - acc: 0.8089 - val_loss: 0.4192 - val_acc: 0.7925\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.3907 - acc: 0.8058 - val_loss: 0.4188 - val_acc: 0.7928\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.3893 - acc: 0.8105 - val_loss: 0.4198 - val_acc: 0.7945\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 3s 731ms/step - loss: 0.3867 - acc: 0.8099 - val_loss: 0.4181 - val_acc: 0.7935\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.3894 - acc: 0.8068 - val_loss: 0.4230 - val_acc: 0.7932\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.3860 - acc: 0.8098 - val_loss: 0.4177 - val_acc: 0.7947\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.3799 - acc: 0.8135 - val_loss: 0.4170 - val_acc: 0.7935\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.3813 - acc: 0.8113 - val_loss: 0.4165 - val_acc: 0.7963\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.3787 - acc: 0.8140 - val_loss: 0.4166 - val_acc: 0.7937\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.3763 - acc: 0.8174 - val_loss: 0.4132 - val_acc: 0.7964\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 3s 709ms/step - loss: 0.3762 - acc: 0.8151 - val_loss: 0.4146 - val_acc: 0.7953\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.3751 - acc: 0.8180 - val_loss: 0.4160 - val_acc: 0.7968\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 3s 685ms/step - loss: 0.3736 - acc: 0.8137 - val_loss: 0.4132 - val_acc: 0.7965\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 3s 588ms/step - loss: 0.3777 - acc: 0.8174 - val_loss: 0.4133 - val_acc: 0.7972\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.3700 - acc: 0.8212 - val_loss: 0.4154 - val_acc: 0.7958\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.3660 - acc: 0.8176 - val_loss: 0.4109 - val_acc: 0.7972\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.3659 - acc: 0.8201 - val_loss: 0.4168 - val_acc: 0.7966\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 716ms/step - loss: 0.3695 - acc: 0.8206 - val_loss: 0.4126 - val_acc: 0.8000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 3s 728ms/step - loss: 0.3658 - acc: 0.8186 - val_loss: 0.4119 - val_acc: 0.7966\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.3676 - acc: 0.8204 - val_loss: 0.4105 - val_acc: 0.8005\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 3s 745ms/step - loss: 0.3668 - acc: 0.8225 - val_loss: 0.4091 - val_acc: 0.7986\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.3579 - acc: 0.8286 - val_loss: 0.4118 - val_acc: 0.7992\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.3630 - acc: 0.8255 - val_loss: 0.4127 - val_acc: 0.8005\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 0.3631 - acc: 0.8242 - val_loss: 0.4157 - val_acc: 0.7948\n",
      "\n",
      " \n",
      "\n",
      "Training model with 30000 training points:\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 6s 575ms/step - loss: 0.6653 - acc: 0.6310 - val_loss: 0.5986 - val_acc: 0.6604\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.5936 - acc: 0.6665 - val_loss: 0.5740 - val_acc: 0.6643\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.5772 - acc: 0.6656 - val_loss: 0.5631 - val_acc: 0.6679\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 0.5640 - acc: 0.6744 - val_loss: 0.5547 - val_acc: 0.6835\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 4s 526ms/step - loss: 0.5529 - acc: 0.6851 - val_loss: 0.5418 - val_acc: 0.6984\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 4s 532ms/step - loss: 0.5455 - acc: 0.6987 - val_loss: 0.5312 - val_acc: 0.7081\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.5340 - acc: 0.7091 - val_loss: 0.5248 - val_acc: 0.7162\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 4s 529ms/step - loss: 0.5295 - acc: 0.7125 - val_loss: 0.5194 - val_acc: 0.7194\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.5214 - acc: 0.7213 - val_loss: 0.5172 - val_acc: 0.7212\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.5182 - acc: 0.7223 - val_loss: 0.5113 - val_acc: 0.7273\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.5116 - acc: 0.7287 - val_loss: 0.5035 - val_acc: 0.7314\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 4s 524ms/step - loss: 0.5057 - acc: 0.7305 - val_loss: 0.4962 - val_acc: 0.7386\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 4s 529ms/step - loss: 0.4995 - acc: 0.7363 - val_loss: 0.4912 - val_acc: 0.7434\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.4955 - acc: 0.7420 - val_loss: 0.4877 - val_acc: 0.7448\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 4s 526ms/step - loss: 0.4935 - acc: 0.7408 - val_loss: 0.4852 - val_acc: 0.7473\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 4s 529ms/step - loss: 0.4897 - acc: 0.7436 - val_loss: 0.4793 - val_acc: 0.7490\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4825 - acc: 0.7484 - val_loss: 0.4776 - val_acc: 0.7499\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.4794 - acc: 0.7505 - val_loss: 0.4776 - val_acc: 0.7532\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 0.4766 - acc: 0.7523 - val_loss: 0.4716 - val_acc: 0.7558\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.4751 - acc: 0.7499 - val_loss: 0.4670 - val_acc: 0.7593\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.4718 - acc: 0.7551 - val_loss: 0.4653 - val_acc: 0.7576\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 4s 529ms/step - loss: 0.4686 - acc: 0.7570 - val_loss: 0.4611 - val_acc: 0.7617\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 4s 526ms/step - loss: 0.4652 - acc: 0.7578 - val_loss: 0.4589 - val_acc: 0.7618\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 4s 523ms/step - loss: 0.4599 - acc: 0.7642 - val_loss: 0.4601 - val_acc: 0.7625\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.4594 - acc: 0.7654 - val_loss: 0.4558 - val_acc: 0.7657\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4568 - acc: 0.7633 - val_loss: 0.4514 - val_acc: 0.7678\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 4s 544ms/step - loss: 0.4529 - acc: 0.7680 - val_loss: 0.4505 - val_acc: 0.7676\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 0.4531 - acc: 0.7674 - val_loss: 0.4473 - val_acc: 0.7701\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4523 - acc: 0.7666 - val_loss: 0.4515 - val_acc: 0.7687\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 0.4475 - acc: 0.7700 - val_loss: 0.4454 - val_acc: 0.7714\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.4494 - acc: 0.7651 - val_loss: 0.4418 - val_acc: 0.7735\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.4412 - acc: 0.7713 - val_loss: 0.4419 - val_acc: 0.7749\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 4s 527ms/step - loss: 0.4398 - acc: 0.7751 - val_loss: 0.4387 - val_acc: 0.7767\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 4s 526ms/step - loss: 0.4397 - acc: 0.7746 - val_loss: 0.4362 - val_acc: 0.7783\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.4395 - acc: 0.7729 - val_loss: 0.4355 - val_acc: 0.7797\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.4323 - acc: 0.7817 - val_loss: 0.4327 - val_acc: 0.7800\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 0.4302 - acc: 0.7816 - val_loss: 0.4322 - val_acc: 0.7805\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4321 - acc: 0.7789 - val_loss: 0.4297 - val_acc: 0.7838\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 0.4304 - acc: 0.7839 - val_loss: 0.4265 - val_acc: 0.7853\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 4s 529ms/step - loss: 0.4286 - acc: 0.7840 - val_loss: 0.4280 - val_acc: 0.7836\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 4s 521ms/step - loss: 0.4194 - acc: 0.7876 - val_loss: 0.4239 - val_acc: 0.7873\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 4s 530ms/step - loss: 0.4158 - acc: 0.7920 - val_loss: 0.4188 - val_acc: 0.7906\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4183 - acc: 0.7902 - val_loss: 0.4183 - val_acc: 0.7916\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 4s 524ms/step - loss: 0.4165 - acc: 0.7884 - val_loss: 0.4190 - val_acc: 0.7924\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 4s 520ms/step - loss: 0.4134 - acc: 0.7916 - val_loss: 0.4143 - val_acc: 0.7946\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 4s 524ms/step - loss: 0.4095 - acc: 0.7958 - val_loss: 0.4128 - val_acc: 0.7959\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 4s 522ms/step - loss: 0.4073 - acc: 0.7976 - val_loss: 0.4121 - val_acc: 0.7955\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 4s 531ms/step - loss: 0.4090 - acc: 0.7940 - val_loss: 0.4136 - val_acc: 0.7944\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 4s 528ms/step - loss: 0.4061 - acc: 0.7974 - val_loss: 0.4127 - val_acc: 0.7956\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 4s 524ms/step - loss: 0.4042 - acc: 0.7980 - val_loss: 0.4116 - val_acc: 0.7946\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 4s 525ms/step - loss: 0.4061 - acc: 0.7970 - val_loss: 0.4103 - val_acc: 0.7949\n",
      "\n",
      " \n",
      "\n",
      "Training model with 50000 training points:\n",
      "Epoch 1/100\n",
      "13/13 [==============================] - 7s 461ms/step - loss: 0.6494 - acc: 0.6561 - val_loss: 0.5821 - val_acc: 0.6626\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.5798 - acc: 0.6676 - val_loss: 0.5622 - val_acc: 0.6666\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.5621 - acc: 0.6760 - val_loss: 0.5475 - val_acc: 0.6890\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 6s 448ms/step - loss: 0.5491 - acc: 0.6932 - val_loss: 0.5323 - val_acc: 0.7045\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 6s 440ms/step - loss: 0.5364 - acc: 0.7076 - val_loss: 0.5213 - val_acc: 0.7159\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 6s 444ms/step - loss: 0.5224 - acc: 0.7192 - val_loss: 0.5088 - val_acc: 0.7281\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 6s 435ms/step - loss: 0.5136 - acc: 0.7262 - val_loss: 0.4995 - val_acc: 0.7366\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.5062 - acc: 0.7318 - val_loss: 0.4924 - val_acc: 0.7418\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.4979 - acc: 0.7376 - val_loss: 0.4838 - val_acc: 0.7477\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.4922 - acc: 0.7419 - val_loss: 0.4798 - val_acc: 0.7517\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.4892 - acc: 0.7446 - val_loss: 0.4716 - val_acc: 0.7551\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.4791 - acc: 0.7505 - val_loss: 0.4669 - val_acc: 0.7560\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 6s 435ms/step - loss: 0.4776 - acc: 0.7513 - val_loss: 0.4632 - val_acc: 0.7598\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 6s 443ms/step - loss: 0.4723 - acc: 0.7552 - val_loss: 0.4616 - val_acc: 0.7605\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 6s 436ms/step - loss: 0.4661 - acc: 0.7560 - val_loss: 0.4553 - val_acc: 0.7650\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 6s 435ms/step - loss: 0.4611 - acc: 0.7574 - val_loss: 0.4500 - val_acc: 0.7689\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 6s 436ms/step - loss: 0.4577 - acc: 0.7638 - val_loss: 0.4465 - val_acc: 0.7702\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.4509 - acc: 0.7662 - val_loss: 0.4446 - val_acc: 0.7733\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 6s 435ms/step - loss: 0.4509 - acc: 0.7681 - val_loss: 0.4404 - val_acc: 0.7740\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 6s 432ms/step - loss: 0.4479 - acc: 0.7682 - val_loss: 0.4354 - val_acc: 0.7789\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.4402 - acc: 0.7737 - val_loss: 0.4308 - val_acc: 0.7826\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.4387 - acc: 0.7771 - val_loss: 0.4274 - val_acc: 0.7840\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 6s 440ms/step - loss: 0.4370 - acc: 0.7780 - val_loss: 0.4287 - val_acc: 0.7850\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.4365 - acc: 0.7776 - val_loss: 0.4284 - val_acc: 0.7819\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 6s 436ms/step - loss: 0.4319 - acc: 0.7810 - val_loss: 0.4188 - val_acc: 0.7906\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 6s 445ms/step - loss: 0.4263 - acc: 0.7862 - val_loss: 0.4139 - val_acc: 0.7947\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 6s 434ms/step - loss: 0.4223 - acc: 0.7900 - val_loss: 0.4104 - val_acc: 0.7976\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 5s 378ms/step - loss: 0.4145 - acc: 0.7935 - val_loss: 0.4071 - val_acc: 0.7996\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 5s 434ms/step - loss: 0.4225 - acc: 0.7879 - val_loss: 0.4083 - val_acc: 0.7990\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.4151 - acc: 0.7906 - val_loss: 0.4059 - val_acc: 0.8006\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.4128 - acc: 0.7929 - val_loss: 0.4026 - val_acc: 0.8041\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.4074 - acc: 0.7985 - val_loss: 0.3979 - val_acc: 0.8046\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.4058 - acc: 0.7995 - val_loss: 0.3987 - val_acc: 0.8040\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.4047 - acc: 0.7994 - val_loss: 0.3930 - val_acc: 0.8072\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.4013 - acc: 0.7996 - val_loss: 0.3927 - val_acc: 0.8089\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.3964 - acc: 0.8044 - val_loss: 0.3899 - val_acc: 0.8106\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 6s 440ms/step - loss: 0.3972 - acc: 0.8060 - val_loss: 0.3896 - val_acc: 0.8112\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.3950 - acc: 0.8033 - val_loss: 0.3888 - val_acc: 0.8095\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.3908 - acc: 0.8092 - val_loss: 0.3896 - val_acc: 0.8104\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 6s 436ms/step - loss: 0.3931 - acc: 0.8064 - val_loss: 0.3901 - val_acc: 0.8085\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.3907 - acc: 0.8072 - val_loss: 0.3865 - val_acc: 0.8113\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 6s 443ms/step - loss: 0.3890 - acc: 0.8061 - val_loss: 0.3833 - val_acc: 0.8124\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 6s 448ms/step - loss: 0.3854 - acc: 0.8094 - val_loss: 0.3839 - val_acc: 0.8135\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.3797 - acc: 0.8153 - val_loss: 0.3833 - val_acc: 0.8122\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.3824 - acc: 0.8144 - val_loss: 0.3809 - val_acc: 0.8150\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.3811 - acc: 0.8120 - val_loss: 0.3831 - val_acc: 0.8126\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 6s 450ms/step - loss: 0.3830 - acc: 0.8125 - val_loss: 0.3827 - val_acc: 0.8136\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.3798 - acc: 0.8144 - val_loss: 0.3814 - val_acc: 0.8148\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 6s 440ms/step - loss: 0.3786 - acc: 0.8153 - val_loss: 0.3780 - val_acc: 0.8162\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 6s 441ms/step - loss: 0.3746 - acc: 0.8177 - val_loss: 0.3767 - val_acc: 0.8179\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.3714 - acc: 0.8202 - val_loss: 0.3768 - val_acc: 0.8169\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 6s 435ms/step - loss: 0.3740 - acc: 0.8177 - val_loss: 0.3778 - val_acc: 0.8157\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 6s 439ms/step - loss: 0.3712 - acc: 0.8180 - val_loss: 0.3755 - val_acc: 0.8171\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 6s 438ms/step - loss: 0.3717 - acc: 0.8181 - val_loss: 0.3780 - val_acc: 0.8138\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 6s 440ms/step - loss: 0.3699 - acc: 0.8185 - val_loss: 0.3769 - val_acc: 0.8160\n",
      "\n",
      " \n",
      "\n",
      "Training model with 100000 training points:\n",
      "Epoch 1/100\n",
      "25/25 [==============================] - 12s 400ms/step - loss: 0.6377 - acc: 0.6273 - val_loss: 0.5664 - val_acc: 0.6617\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.5637 - acc: 0.6730 - val_loss: 0.5377 - val_acc: 0.7036\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.5398 - acc: 0.7046 - val_loss: 0.5140 - val_acc: 0.7261\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.5220 - acc: 0.7178 - val_loss: 0.4938 - val_acc: 0.7387\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.5034 - acc: 0.7323 - val_loss: 0.4801 - val_acc: 0.7492\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 10s 384ms/step - loss: 0.4919 - acc: 0.7394 - val_loss: 0.4706 - val_acc: 0.7549\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 10s 384ms/step - loss: 0.4801 - acc: 0.7475 - val_loss: 0.4582 - val_acc: 0.7646\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 10s 389ms/step - loss: 0.4726 - acc: 0.7538 - val_loss: 0.4519 - val_acc: 0.7673\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.4646 - acc: 0.7598 - val_loss: 0.4461 - val_acc: 0.7722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.4578 - acc: 0.7624 - val_loss: 0.4359 - val_acc: 0.7780\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.4495 - acc: 0.7672 - val_loss: 0.4296 - val_acc: 0.7834\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.4419 - acc: 0.7717 - val_loss: 0.4245 - val_acc: 0.7883\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.4352 - acc: 0.7798 - val_loss: 0.4183 - val_acc: 0.7912\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.4333 - acc: 0.7796 - val_loss: 0.4128 - val_acc: 0.7959\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 10s 384ms/step - loss: 0.4272 - acc: 0.7834 - val_loss: 0.4087 - val_acc: 0.8000\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.4236 - acc: 0.7868 - val_loss: 0.4014 - val_acc: 0.8043\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.4138 - acc: 0.7943 - val_loss: 0.3931 - val_acc: 0.8086\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 10s 384ms/step - loss: 0.4061 - acc: 0.7988 - val_loss: 0.3911 - val_acc: 0.8087\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 9s 380ms/step - loss: 0.4018 - acc: 0.8020 - val_loss: 0.3850 - val_acc: 0.8128\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.4008 - acc: 0.7999 - val_loss: 0.3826 - val_acc: 0.8139\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 10s 389ms/step - loss: 0.3956 - acc: 0.8046 - val_loss: 0.3799 - val_acc: 0.8143\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3941 - acc: 0.8062 - val_loss: 0.3819 - val_acc: 0.8141\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.3907 - acc: 0.8083 - val_loss: 0.3778 - val_acc: 0.8170\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 9s 351ms/step - loss: 0.3883 - acc: 0.8084 - val_loss: 0.3728 - val_acc: 0.8196\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.3862 - acc: 0.8098 - val_loss: 0.3689 - val_acc: 0.8210\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.3841 - acc: 0.8128 - val_loss: 0.3776 - val_acc: 0.8171\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.3805 - acc: 0.8139 - val_loss: 0.3670 - val_acc: 0.8226\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.3780 - acc: 0.8156 - val_loss: 0.3661 - val_acc: 0.8226\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.3740 - acc: 0.8185 - val_loss: 0.3648 - val_acc: 0.8240\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3742 - acc: 0.8176 - val_loss: 0.3629 - val_acc: 0.8248\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3734 - acc: 0.8189 - val_loss: 0.3605 - val_acc: 0.8260\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3680 - acc: 0.8210 - val_loss: 0.3601 - val_acc: 0.8263\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 10s 389ms/step - loss: 0.3674 - acc: 0.8228 - val_loss: 0.3595 - val_acc: 0.8270\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.3667 - acc: 0.8226 - val_loss: 0.3596 - val_acc: 0.8259\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.3647 - acc: 0.8227 - val_loss: 0.3590 - val_acc: 0.8265\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3663 - acc: 0.8209 - val_loss: 0.3566 - val_acc: 0.8275\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 10s 383ms/step - loss: 0.3631 - acc: 0.8252 - val_loss: 0.3572 - val_acc: 0.8270\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.3656 - acc: 0.8227 - val_loss: 0.3586 - val_acc: 0.8280\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 10s 383ms/step - loss: 0.3624 - acc: 0.8241 - val_loss: 0.3559 - val_acc: 0.8289\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 10s 385ms/step - loss: 0.3587 - acc: 0.8259 - val_loss: 0.3540 - val_acc: 0.8285\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 9s 382ms/step - loss: 0.3552 - acc: 0.8283 - val_loss: 0.3542 - val_acc: 0.8292\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3584 - acc: 0.8266 - val_loss: 0.3525 - val_acc: 0.8311\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.3571 - acc: 0.8269 - val_loss: 0.3535 - val_acc: 0.8289\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 10s 387ms/step - loss: 0.3578 - acc: 0.8273 - val_loss: 0.3541 - val_acc: 0.8279\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 10s 388ms/step - loss: 0.3510 - acc: 0.8299 - val_loss: 0.3518 - val_acc: 0.8302\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 10s 391ms/step - loss: 0.3551 - acc: 0.8271 - val_loss: 0.3528 - val_acc: 0.8301\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.3524 - acc: 0.8301 - val_loss: 0.3517 - val_acc: 0.8304\n",
      "\n",
      " \n",
      "\n",
      "Training model with 200000 training points:\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 19s 364ms/step - loss: 0.6129 - acc: 0.6519 - val_loss: 0.5363 - val_acc: 0.7036\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.5369 - acc: 0.7037 - val_loss: 0.4993 - val_acc: 0.7349\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.5058 - acc: 0.7301 - val_loss: 0.4675 - val_acc: 0.7556\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.4826 - acc: 0.7454 - val_loss: 0.4497 - val_acc: 0.7668\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 18s 360ms/step - loss: 0.4689 - acc: 0.7554 - val_loss: 0.4376 - val_acc: 0.7767\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 18s 361ms/step - loss: 0.4542 - acc: 0.7666 - val_loss: 0.4248 - val_acc: 0.7862\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 17s 356ms/step - loss: 0.4430 - acc: 0.7737 - val_loss: 0.4084 - val_acc: 0.7972\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.4286 - acc: 0.7832 - val_loss: 0.3997 - val_acc: 0.8043\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 17s 355ms/step - loss: 0.4194 - acc: 0.7901 - val_loss: 0.3905 - val_acc: 0.8102\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 17s 341ms/step - loss: 0.4083 - acc: 0.7970 - val_loss: 0.3815 - val_acc: 0.8155\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.4006 - acc: 0.8014 - val_loss: 0.3789 - val_acc: 0.8153\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3953 - acc: 0.8047 - val_loss: 0.3730 - val_acc: 0.8189\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 18s 361ms/step - loss: 0.3903 - acc: 0.8078 - val_loss: 0.3679 - val_acc: 0.8217\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3876 - acc: 0.8105 - val_loss: 0.3642 - val_acc: 0.8231\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 17s 358ms/step - loss: 0.3829 - acc: 0.8135 - val_loss: 0.3633 - val_acc: 0.8243\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 17s 358ms/step - loss: 0.3765 - acc: 0.8159 - val_loss: 0.3611 - val_acc: 0.8275\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 17s 356ms/step - loss: 0.3779 - acc: 0.8161 - val_loss: 0.3583 - val_acc: 0.8278\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3753 - acc: 0.8174 - val_loss: 0.3575 - val_acc: 0.8269\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 18s 360ms/step - loss: 0.3724 - acc: 0.8197 - val_loss: 0.3575 - val_acc: 0.8282\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 18s 362ms/step - loss: 0.3708 - acc: 0.8200 - val_loss: 0.3536 - val_acc: 0.8303\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3682 - acc: 0.8229 - val_loss: 0.3533 - val_acc: 0.8307\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 17s 358ms/step - loss: 0.3659 - acc: 0.8232 - val_loss: 0.3539 - val_acc: 0.8306\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3662 - acc: 0.8234 - val_loss: 0.3516 - val_acc: 0.8300\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3616 - acc: 0.8261 - val_loss: 0.3505 - val_acc: 0.8321\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3610 - acc: 0.8266 - val_loss: 0.3497 - val_acc: 0.8321\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3604 - acc: 0.8267 - val_loss: 0.3504 - val_acc: 0.8312\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 18s 361ms/step - loss: 0.3607 - acc: 0.8255 - val_loss: 0.3481 - val_acc: 0.8324\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 17s 356ms/step - loss: 0.3564 - acc: 0.8277 - val_loss: 0.3458 - val_acc: 0.8338\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 17s 351ms/step - loss: 0.3566 - acc: 0.8280 - val_loss: 0.3452 - val_acc: 0.8345\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3546 - acc: 0.8289 - val_loss: 0.3481 - val_acc: 0.8327\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 17s 358ms/step - loss: 0.3550 - acc: 0.8287 - val_loss: 0.3460 - val_acc: 0.8339\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 17s 343ms/step - loss: 0.3550 - acc: 0.8286 - val_loss: 0.3429 - val_acc: 0.8349\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3525 - acc: 0.8303 - val_loss: 0.3439 - val_acc: 0.8339\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3511 - acc: 0.8310 - val_loss: 0.3428 - val_acc: 0.8342\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3514 - acc: 0.8301 - val_loss: 0.3429 - val_acc: 0.8346\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 17s 356ms/step - loss: 0.3521 - acc: 0.8307 - val_loss: 0.3434 - val_acc: 0.8352\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3509 - acc: 0.8311 - val_loss: 0.3407 - val_acc: 0.8360\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 17s 355ms/step - loss: 0.3511 - acc: 0.8316 - val_loss: 0.3419 - val_acc: 0.8357\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 17s 356ms/step - loss: 0.3504 - acc: 0.8311 - val_loss: 0.3410 - val_acc: 0.8357\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 17s 357ms/step - loss: 0.3495 - acc: 0.8302 - val_loss: 0.3407 - val_acc: 0.8365\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3481 - acc: 0.8323 - val_loss: 0.3404 - val_acc: 0.8360\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 17s 355ms/step - loss: 0.3470 - acc: 0.8331 - val_loss: 0.3401 - val_acc: 0.8369\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3461 - acc: 0.8329 - val_loss: 0.3403 - val_acc: 0.8363\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3444 - acc: 0.8337 - val_loss: 0.3377 - val_acc: 0.8382\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3440 - acc: 0.8354 - val_loss: 0.3409 - val_acc: 0.8361\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 17s 358ms/step - loss: 0.3432 - acc: 0.8351 - val_loss: 0.3398 - val_acc: 0.8364\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 18s 358ms/step - loss: 0.3423 - acc: 0.8365 - val_loss: 0.3384 - val_acc: 0.8369\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3419 - acc: 0.8351 - val_loss: 0.3385 - val_acc: 0.8375\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 18s 359ms/step - loss: 0.3422 - acc: 0.8355 - val_loss: 0.3400 - val_acc: 0.8367\n",
      "\n",
      " \n",
      "\n",
      "Training model with 500000 training points:\n",
      "Epoch 1/100\n",
      "123/123 [==============================] - 43s 336ms/step - loss: 0.5888 - acc: 0.6623 - val_loss: 0.4903 - val_acc: 0.7437\n",
      "Epoch 2/100\n",
      "123/123 [==============================] - 41s 334ms/step - loss: 0.4918 - acc: 0.7407 - val_loss: 0.4474 - val_acc: 0.7714\n",
      "Epoch 3/100\n",
      "123/123 [==============================] - 41s 331ms/step - loss: 0.4525 - acc: 0.7664 - val_loss: 0.4196 - val_acc: 0.7936\n",
      "Epoch 4/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.4242 - acc: 0.7863 - val_loss: 0.3827 - val_acc: 0.8156\n",
      "Epoch 5/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.4019 - acc: 0.8009 - val_loss: 0.3743 - val_acc: 0.8178\n",
      "Epoch 6/100\n",
      "123/123 [==============================] - 42s 338ms/step - loss: 0.3901 - acc: 0.8092 - val_loss: 0.3697 - val_acc: 0.8201\n",
      "Epoch 7/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3831 - acc: 0.8130 - val_loss: 0.3574 - val_acc: 0.8279\n",
      "Epoch 8/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3746 - acc: 0.8186 - val_loss: 0.3556 - val_acc: 0.8288\n",
      "Epoch 9/100\n",
      "123/123 [==============================] - 41s 334ms/step - loss: 0.3708 - acc: 0.8205 - val_loss: 0.3513 - val_acc: 0.8307\n",
      "Epoch 10/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3668 - acc: 0.8220 - val_loss: 0.3468 - val_acc: 0.8339\n",
      "Epoch 11/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3642 - acc: 0.8243 - val_loss: 0.3464 - val_acc: 0.8337\n",
      "Epoch 12/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3612 - acc: 0.8261 - val_loss: 0.3469 - val_acc: 0.8329\n",
      "Epoch 13/100\n",
      "123/123 [==============================] - 41s 331ms/step - loss: 0.3599 - acc: 0.8256 - val_loss: 0.3439 - val_acc: 0.8336\n",
      "Epoch 14/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3601 - acc: 0.8261 - val_loss: 0.3428 - val_acc: 0.8357\n",
      "Epoch 15/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3571 - acc: 0.8276 - val_loss: 0.3438 - val_acc: 0.8360\n",
      "Epoch 16/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3544 - acc: 0.8291 - val_loss: 0.3392 - val_acc: 0.8365\n",
      "Epoch 17/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3548 - acc: 0.8292 - val_loss: 0.3393 - val_acc: 0.8360\n",
      "Epoch 18/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3536 - acc: 0.8298 - val_loss: 0.3373 - val_acc: 0.8371\n",
      "Epoch 19/100\n",
      "123/123 [==============================] - 41s 333ms/step - loss: 0.3514 - acc: 0.8302 - val_loss: 0.3348 - val_acc: 0.8394\n",
      "Epoch 20/100\n",
      "123/123 [==============================] - 41s 334ms/step - loss: 0.3491 - acc: 0.8320 - val_loss: 0.3343 - val_acc: 0.8388\n",
      "Epoch 21/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3490 - acc: 0.8323 - val_loss: 0.3366 - val_acc: 0.8381\n",
      "Epoch 22/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3474 - acc: 0.8330 - val_loss: 0.3346 - val_acc: 0.8386\n",
      "Epoch 23/100\n",
      "123/123 [==============================] - 41s 331ms/step - loss: 0.3475 - acc: 0.8326 - val_loss: 0.3359 - val_acc: 0.8383\n",
      "Epoch 24/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3469 - acc: 0.8320 - val_loss: 0.3341 - val_acc: 0.8396\n",
      "Epoch 25/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3452 - acc: 0.8332 - val_loss: 0.3345 - val_acc: 0.8383\n",
      "Epoch 26/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3446 - acc: 0.8339 - val_loss: 0.3342 - val_acc: 0.8393\n",
      "Epoch 27/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3441 - acc: 0.8353 - val_loss: 0.3327 - val_acc: 0.8394\n",
      "Epoch 28/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3429 - acc: 0.8351 - val_loss: 0.3331 - val_acc: 0.8385\n",
      "Epoch 29/100\n",
      "123/123 [==============================] - 42s 338ms/step - loss: 0.3449 - acc: 0.8340 - val_loss: 0.3309 - val_acc: 0.8401\n",
      "Epoch 30/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3425 - acc: 0.8362 - val_loss: 0.3317 - val_acc: 0.8400\n",
      "Epoch 31/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3411 - acc: 0.8370 - val_loss: 0.3309 - val_acc: 0.8403\n",
      "Epoch 32/100\n",
      "123/123 [==============================] - 41s 332ms/step - loss: 0.3412 - acc: 0.8359 - val_loss: 0.3280 - val_acc: 0.8421\n",
      "Epoch 33/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3419 - acc: 0.8355 - val_loss: 0.3289 - val_acc: 0.8421\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3418 - acc: 0.8357 - val_loss: 0.3284 - val_acc: 0.8414\n",
      "Epoch 35/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3399 - acc: 0.8373 - val_loss: 0.3302 - val_acc: 0.8408\n",
      "Epoch 36/100\n",
      "123/123 [==============================] - 41s 335ms/step - loss: 0.3388 - acc: 0.8373 - val_loss: 0.3281 - val_acc: 0.8413\n",
      "Epoch 37/100\n",
      "123/123 [==============================] - 41s 337ms/step - loss: 0.3387 - acc: 0.8373 - val_loss: 0.3304 - val_acc: 0.8400\n",
      "Epoch 38/100\n",
      "123/123 [==============================] - 41s 336ms/step - loss: 0.3402 - acc: 0.8364 - val_loss: 0.3288 - val_acc: 0.8415\n",
      "\n",
      " \n",
      "\n",
      "Training model with 1000000 training points:\n",
      "Epoch 1/100\n",
      "245/245 [==============================] - 83s 332ms/step - loss: 0.5535 - acc: 0.6965 - val_loss: 0.4415 - val_acc: 0.7736\n",
      "Epoch 2/100\n",
      "245/245 [==============================] - 80s 327ms/step - loss: 0.4465 - acc: 0.7712 - val_loss: 0.3834 - val_acc: 0.8121\n",
      "Epoch 3/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3996 - acc: 0.8030 - val_loss: 0.3627 - val_acc: 0.8249\n",
      "Epoch 4/100\n",
      "245/245 [==============================] - 81s 332ms/step - loss: 0.3804 - acc: 0.8144 - val_loss: 0.3537 - val_acc: 0.8300\n",
      "Epoch 5/100\n",
      "245/245 [==============================] - 81s 329ms/step - loss: 0.3701 - acc: 0.8205 - val_loss: 0.3477 - val_acc: 0.8330\n",
      "Epoch 6/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3648 - acc: 0.8237 - val_loss: 0.3469 - val_acc: 0.8352\n",
      "Epoch 7/100\n",
      "245/245 [==============================] - 80s 327ms/step - loss: 0.3604 - acc: 0.8263 - val_loss: 0.3429 - val_acc: 0.8355\n",
      "Epoch 8/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3572 - acc: 0.8275 - val_loss: 0.3397 - val_acc: 0.8374\n",
      "Epoch 9/100\n",
      "245/245 [==============================] - 81s 329ms/step - loss: 0.3546 - acc: 0.8290 - val_loss: 0.3376 - val_acc: 0.8390\n",
      "Epoch 10/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3527 - acc: 0.8302 - val_loss: 0.3339 - val_acc: 0.8400\n",
      "Epoch 11/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3505 - acc: 0.8311 - val_loss: 0.3356 - val_acc: 0.8404\n",
      "Epoch 12/100\n",
      "245/245 [==============================] - 80s 327ms/step - loss: 0.3483 - acc: 0.8320 - val_loss: 0.3325 - val_acc: 0.8414\n",
      "Epoch 13/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3463 - acc: 0.8331 - val_loss: 0.3326 - val_acc: 0.8402\n",
      "Epoch 14/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3464 - acc: 0.8335 - val_loss: 0.3309 - val_acc: 0.8411\n",
      "Epoch 15/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3449 - acc: 0.8341 - val_loss: 0.3294 - val_acc: 0.8419\n",
      "Epoch 16/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3435 - acc: 0.8348 - val_loss: 0.3306 - val_acc: 0.8430\n",
      "Epoch 17/100\n",
      "245/245 [==============================] - 80s 326ms/step - loss: 0.3438 - acc: 0.8344 - val_loss: 0.3286 - val_acc: 0.8419\n",
      "Epoch 18/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3413 - acc: 0.8363 - val_loss: 0.3274 - val_acc: 0.8428\n",
      "Epoch 19/100\n",
      "245/245 [==============================] - 81s 332ms/step - loss: 0.3399 - acc: 0.8368 - val_loss: 0.3258 - val_acc: 0.8430\n",
      "Epoch 20/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3401 - acc: 0.8365 - val_loss: 0.3253 - val_acc: 0.8435\n",
      "Epoch 21/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3383 - acc: 0.8378 - val_loss: 0.3250 - val_acc: 0.8445\n",
      "Epoch 22/100\n",
      "245/245 [==============================] - 80s 327ms/step - loss: 0.3385 - acc: 0.8375 - val_loss: 0.3242 - val_acc: 0.8443\n",
      "Epoch 23/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3373 - acc: 0.8379 - val_loss: 0.3235 - val_acc: 0.8455\n",
      "Epoch 24/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3373 - acc: 0.8381 - val_loss: 0.3223 - val_acc: 0.8464\n",
      "Epoch 25/100\n",
      "245/245 [==============================] - 81s 331ms/step - loss: 0.3382 - acc: 0.8374 - val_loss: 0.3228 - val_acc: 0.8457\n",
      "Epoch 26/100\n",
      "245/245 [==============================] - 81s 329ms/step - loss: 0.3354 - acc: 0.8390 - val_loss: 0.3244 - val_acc: 0.8447\n",
      "Epoch 27/100\n",
      "245/245 [==============================] - 81s 329ms/step - loss: 0.3351 - acc: 0.8393 - val_loss: 0.3244 - val_acc: 0.8448\n",
      "Epoch 28/100\n",
      "245/245 [==============================] - 81s 330ms/step - loss: 0.3365 - acc: 0.8387 - val_loss: 0.3225 - val_acc: 0.8453\n",
      "Epoch 29/100\n",
      "245/245 [==============================] - 80s 328ms/step - loss: 0.3351 - acc: 0.8394 - val_loss: 0.3220 - val_acc: 0.8463\n",
      "\n",
      " \n",
      "\n",
      "Training model with 3339495 training points:\n",
      "Epoch 1/100\n",
      "816/816 [==============================] - 268s 327ms/step - loss: 0.4942 - acc: 0.7355 - val_loss: 0.3575 - val_acc: 0.8293\n",
      "Epoch 2/100\n",
      "816/816 [==============================] - 267s 328ms/step - loss: 0.3719 - acc: 0.8195 - val_loss: 0.3413 - val_acc: 0.8370\n",
      "Epoch 3/100\n",
      "816/816 [==============================] - 266s 327ms/step - loss: 0.3566 - acc: 0.8278 - val_loss: 0.3339 - val_acc: 0.8400\n",
      "Epoch 4/100\n",
      "816/816 [==============================] - 261s 320ms/step - loss: 0.3493 - acc: 0.8318 - val_loss: 0.3287 - val_acc: 0.8423\n",
      "Epoch 5/100\n",
      "816/816 [==============================] - 264s 324ms/step - loss: 0.3457 - acc: 0.8337 - val_loss: 0.3260 - val_acc: 0.8440\n",
      "Epoch 6/100\n",
      "816/816 [==============================] - 269s 329ms/step - loss: 0.3430 - acc: 0.8348 - val_loss: 0.3244 - val_acc: 0.8446\n",
      "Epoch 7/100\n",
      "816/816 [==============================] - 268s 329ms/step - loss: 0.3402 - acc: 0.8364 - val_loss: 0.3234 - val_acc: 0.8459\n",
      "Epoch 8/100\n",
      "816/816 [==============================] - 268s 328ms/step - loss: 0.3381 - acc: 0.8377 - val_loss: 0.3211 - val_acc: 0.8463\n",
      "Epoch 9/100\n",
      "816/816 [==============================] - 267s 328ms/step - loss: 0.3367 - acc: 0.8382 - val_loss: 0.3185 - val_acc: 0.8475\n",
      "Epoch 10/100\n",
      "816/816 [==============================] - 269s 329ms/step - loss: 0.3351 - acc: 0.8394 - val_loss: 0.3195 - val_acc: 0.8466\n",
      "Epoch 11/100\n",
      "816/816 [==============================] - 269s 329ms/step - loss: 0.3344 - acc: 0.8394 - val_loss: 0.3174 - val_acc: 0.8486\n",
      "Epoch 12/100\n",
      "816/816 [==============================] - 269s 329ms/step - loss: 0.3336 - acc: 0.8402 - val_loss: 0.3167 - val_acc: 0.8494\n",
      "Epoch 13/100\n",
      "816/816 [==============================] - 268s 329ms/step - loss: 0.3322 - acc: 0.8407 - val_loss: 0.3156 - val_acc: 0.8495\n",
      "Epoch 14/100\n",
      "816/816 [==============================] - 267s 328ms/step - loss: 0.3313 - acc: 0.8412 - val_loss: 0.3151 - val_acc: 0.8496\n",
      "Epoch 15/100\n",
      "816/816 [==============================] - 268s 328ms/step - loss: 0.3308 - acc: 0.8417 - val_loss: 0.3152 - val_acc: 0.8504\n",
      "Epoch 16/100\n",
      "816/816 [==============================] - 267s 327ms/step - loss: 0.3306 - acc: 0.8416 - val_loss: 0.3127 - val_acc: 0.8516\n",
      "Epoch 17/100\n",
      "816/816 [==============================] - 267s 327ms/step - loss: 0.3297 - acc: 0.8418 - val_loss: 0.3144 - val_acc: 0.8514\n",
      "Epoch 18/100\n",
      "816/816 [==============================] - 267s 327ms/step - loss: 0.3292 - acc: 0.8423 - val_loss: 0.3151 - val_acc: 0.8507\n",
      "Epoch 19/100\n",
      "816/816 [==============================] - 268s 328ms/step - loss: 0.3288 - acc: 0.8428 - val_loss: 0.3125 - val_acc: 0.8507\n",
      "Epoch 20/100\n",
      "275/816 [=========>....................] - ETA: 2:57 - loss: 0.3271 - acc: 0.8440"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_57648/111791934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mstop_early\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     history = model.fit(x_sample,\n\u001b[0m\u001b[1;32m     28\u001b[0m                         \u001b[0my_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/UKAEA/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_size = [1000, 2000, 5000,\n",
    "                10_000, 12_500, 15_000, 20_000, 30_000, 50_000,\n",
    "                 100_000, 200_000, 500_000,\n",
    "                 1_000_000, x_train.shape[0]]\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "permuted_idx = np.random.permutation(x_train.shape[0])\n",
    "permuted_x = x_train[permuted_idx]\n",
    "permuted_y = Y_train[permuted_idx]\n",
    "\n",
    "for size in training_size:\n",
    "\n",
    "    x_sample = permuted_x[:size]\n",
    "    y_sample = permuted_y[:size]\n",
    "    \n",
    "    batch_size = size if size <= 10_000 else 4096\n",
    "    patience = 25 if size <= 10_000 else 5\n",
    "    \n",
    "    print(f\"Training model with {size} training points:\")\n",
    "    model = nn_classifier_big()\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = 'acc')\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_acc', patience = patience)\n",
    "    history = model.fit(x_sample,\n",
    "                        y_sample,\n",
    "                        validation_data = (x_val_sample, y_val_sample),\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = 100,\n",
    "                        callbacks = [stop_early])\n",
    "\n",
    "    train_losses.append(history.history['loss'])\n",
    "    val_losses.append(history.history['val_loss'])\n",
    "    train_accuracies.append(history.history['acc'])\n",
    "    val_accuracies.append(history.history['val_acc'])\n",
    "    \n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ac942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loss_ = [x[-1] for x in val_losses]\n",
    "#acc_ = [x[-1] for x in val_accuracies]\n",
    "acc_ = []\n",
    "loss_ = []\n",
    "for i in range(len(val_losses)):\n",
    "    acc_idx = np.argmax(np.array(val_accuracies[i]))\n",
    "    acc_.append(val_accuracies[i][acc_idx])\n",
    "    loss_.append(val_losses[i][acc_idx])\n",
    "\n",
    "trainable_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
    "ratio = training_size / trainable_params\n",
    "\n",
    "\n",
    "mosaic = \"\"\"\n",
    "    A\n",
    "    A\n",
    "    B\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure(constrained_layout= True, figsize = (12,8))\n",
    "\n",
    "ax_dict = fig.subplot_mosaic(mosaic)\n",
    "\n",
    "ax_dict[\"A\"].plot(training_size, acc_, \"-o\")\n",
    "ax_dict[\"A\"].set_xscale(\"log\");\n",
    "ax_dict[\"A\"].set_ylabel(\"Validation Accuracy\", size = 18);\n",
    "ax_dict[\"A\"].grid()\n",
    "\n",
    "ax_dict[\"B\"].plot(training_size, ratio, \"-o\", color = \"orange\")\n",
    "ax_dict[\"B\"].set_xscale(\"log\");\n",
    "ax_dict[\"B\"].set_yscale(\"log\");\n",
    "ax_dict[\"B\"].hlines(1, training_size[0], training_size[-1], linestyles = \"dashed\", color = \"k\")\n",
    "ax_dict[\"B\"].grid()\n",
    "ax_dict[\"B\"].set_ylabel(r\"$\\frac{Training\\ Points}{Number\\ of\\ Parameters}$\", size = 18);\n",
    "\n",
    "plt.xlabel(\"Number of Training Points\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7905158",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train, columns = train_keys)\n",
    "x_val_df = pd.DataFrame(x_val, columns = train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dce60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: model.predict(x)\n",
    "med = np.median(x_train, axis = 0).reshape((1,x_train.shape[1]))\n",
    "\n",
    "explainer = shap.Explainer(f, med)\n",
    "shap_values = explainer(x_val_df.iloc[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ab770",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a5a7e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ane',\n",
       " 'ate',\n",
       " 'autor',\n",
       " 'machtor',\n",
       " 'x',\n",
       " 'zeff',\n",
       " 'gammae',\n",
       " 'q',\n",
       " 'smag',\n",
       " 'alpha',\n",
       " 'ani1',\n",
       " 'ati0',\n",
       " 'normni1',\n",
       " 'ti_te0',\n",
       " 'lognustar']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c08ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
