{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from scripts.utils import ScaleData, train_keys, target_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "For now we will only look at a pipeline for the ITG flux. Starting with data that does give a QuaLiKiz result, run the ITG classifier and for data that is unstable use the QLKNN regressor to predict the flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<<<<<< local\n",
    "train_data = pd.read_pickle(\"/home/tmadula/data/UKAEA/train_data_clipped.pkl\").sample(1_000_000)\n",
    "validation_data = pd.read_pickle(\"/home/tmadula/data/UKAEA/valid_data_clipped.pkl\")\n",
    "=======\n",
    "train_data = pd.read_pickle(\"/unix/atlastracking/jbarr/train_data_clipped.pkl\")\n",
    "validation_data = pd.read_pickle(\"/unix/atlastracking/jbarr/valid_data_clipped.pkl\")\n",
    ">>>>>>> remote\n",
    "\n",
    "# Keep only the data that gives an output\n",
    "train_data = train_data[train_data[\"target\"] == 1]\n",
    "validation_data = validation_data[validation_data[\"target\"] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data[\"efiitg_gb\"] != 0][\"efiitg_gb\"].hist(bins=100)\n",
    "print(len(train_data[train_data[\"efiitg_gb\"] != 0]) / len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ITG Models for Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "class ITG_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(15, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = self.model(x)\n",
    "        return X\n",
    "\n",
    "class ITG_Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(15, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = self.model(x.float())\n",
    "        return X\n",
    "\n",
    "class ITGDataset(Dataset):\n",
    "    def __init__(self, X, y, z = None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        if self.z is not None:\n",
    "            return[self.X[idx], self.y[idx], self.z[idx]]\n",
    "        else:\n",
    "            return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # add method to add a new row to the dataset\n",
    "    def add(self, x, y, z = None):\n",
    "        self.X = np.append(self.X, x, axis = 0)\n",
    "        self.y = np.append(self.y, y, axis = 0)\n",
    "        \n",
    "        if z is not None:\n",
    "            self.z = np.append(self.z, z, axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrain Classifier\n",
    "We will pretrain the classifier and then freeze the weights so that we only update the regressor during the active learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_keys = train_keys + [\"efiitg_gb\"]\n",
    "\n",
    "train_data = train_data[keep_keys]\n",
    "validation_data = validation_data[keep_keys]\n",
    "\n",
    "nt, nv = train_data.shape[0], validation_data.shape[0]\n",
    "nt_nan, nv_nan =  train_data['efiitg_gb'].isna().sum(), validation_data['efiitg_gb'].isna().sum()\n",
    "\n",
    "train_data = train_data.dropna()\n",
    "validation_data = validation_data.dropna()\n",
    "\n",
    "assert train_data.shape[0] + nt_nan == nt\n",
    "assert validation_data.shape[0] + nv_nan == nv\n",
    "\n",
    "train_data['itg'] = np.where(train_data['efiitg_gb'] != 0, 1, 0)\n",
    "validation_data['itg'] = np.where(validation_data['efiitg_gb'] != 0, 1, 0)\n",
    "\n",
    "assert len(train_data['itg'].unique()) == 2\n",
    "assert len(validation_data['itg'].unique()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_data = train_data.sample(1_00_000)\n",
    "\n",
    "x_train_pre = pre_train_data[train_keys].to_numpy()\n",
    "y_train_class_pre = pre_train_data['itg'].to_numpy()\n",
    "y_train_reg_pre =  pre_train_data['efiitg_gb'].to_numpy()\n",
    "\n",
    "x_val = validation_data[train_keys].to_numpy()\n",
    "y_val_class = validation_data['itg'].to_numpy()\n",
    "y_val_reg =  validation_data['efiitg_gb'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_pre = scaler.fit_transform(x_train_pre)\n",
    "x_val = scaler.transform(x_val)\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "\n",
    "y_train_reg_pre = scaler_reg.fit_transform(y_train_reg_pre.reshape(-1,1))\n",
    "y_val_reg = scaler_reg.transform(y_val_reg.reshape(-1,1))\n",
    "\n",
    "training = ITGDataset(x_train_pre, y_train_class_pre)\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size=1024, shuffle=True)\n",
    "\n",
    "validation = ITGDataset(x_val, y_val_class)\n",
    "validation_dataloader = DataLoader(validation, batch_size=1024, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ITG_Classifier()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1).float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch == num_batches - 1:\n",
    "            loss = loss.item()\n",
    "            print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(-1).float()).item()\n",
    "            # calculate test accuracy\n",
    "            pred_class = torch.round(pred.squeeze())\n",
    "            correct += torch.sum(pred_class == y.float()).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.211359\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.223534 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.184894\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.186582 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.135297\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.167455 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.162810\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.156361 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.152792\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.149418 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.118867\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.144331 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.130077\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.140384 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.146447\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.136170 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.140767\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.132525 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.132538\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.130903 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, classifier, loss_fn, optimizer)\n",
    "    test_loop(validation_dataloader, classifier, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily save model\n",
    "torch.save(classifier.state_dict(), \"classifier_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load classifier\n",
    "classifier = ITG_Classifier()\n",
    "classifier.load_state_dict(torch.load(\"classifier_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get confusion matrix\n",
    "y_pred = classifier(torch.tensor(x_val).float())\n",
    "y_pred = torch.round(y_pred.squeeze())\n",
    "y_pred = y_pred.detach().numpy()\n",
    "\n",
    "con_mat = confusion_matrix(y_val_class, y_pred)\n",
    "con_mat = con_mat / con_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(con_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Regressor\n",
    "\n",
    "Now we will pretrain the regressor on a very small subset of the data and use that as our base for the active learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_train = ITGDataset(x_train_pre, y_train_class_pre, y_train_reg_pre)\n",
    "\n",
    "reg_train_dataloader = DataLoader(reg_train, batch_size=1000, shuffle=True)\n",
    "\n",
    "reg_val = ITGDataset(x_val, y_val_class, y_val_reg)\n",
    "reg_val_dataloader = DataLoader(reg_val, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    for batch, (X, y, z) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, z.float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch == num_batches - 1:\n",
    "            loss = loss.item()\n",
    "            print(f\"loss: {loss:>7f}\")\n",
    "\n",
    "def regression_test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, z in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, z.float()).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error - avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ITG_Regressor()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "MSE_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(regressor.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    regression_train_loop(reg_train_dataloader, regressor, MSE_loss, optimizer)\n",
    "    regression_test_loop(reg_val_dataloader, regressor, MSE_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporarily save model\n",
    "torch.save(regressor.state_dict(), \"regression_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regression model\n",
    "regressor = ITG_Regressor()\n",
    "regressor.load_state_dict(torch.load(\"regression_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted values\n",
    "y_pred = regressor(torch.tensor(x_val).float()).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(y_val_reg, bins=100, density = True, histtype = 'step', label=\"Actual\");\n",
    "\n",
    "plt.hist(y_pred, bins=150, density=True, histtype='step', label='predicted');\n",
    "\n",
    "plt.legend();\n",
    "plt.xlim(-1,6)\n",
    "\n",
    "# plot scatter plot of predicted vs actual\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(y_val_reg, y_pred, s=1, alpha=0.5, label = \"Predicted vs Actual\");\n",
    "\n",
    "min_x = np.min(y_val_reg)\n",
    "\n",
    "# get line of best fit\n",
    "m, b = np.polyfit(y_val_reg.squeeze(), y_pred, 1)\n",
    "plt.plot(np.linspace(min_x,6,100), m*np.linspace(min_x,6,100)+b, 'r-', label=f'y = {m[0]:.2f}x + {b[0]:.2f}');\n",
    "\n",
    "plt.plot([min_x,6],[min_x,6], 'k--', label = 'y = x');\n",
    "plt.xlim(min_x - 0.1,6)\n",
    "plt.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Learning\n",
    "\n",
    "First pass, will put everything into nice functions after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2688\n"
     ]
    }
   ],
   "source": [
    "x_train = train_data[train_keys].to_numpy()\n",
    "y_train_class = train_data['itg'].to_numpy()\n",
    "y_train_reg =  train_data['efiitg_gb'].to_numpy()\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "y_train_reg = scaler_reg.transform(y_train_reg.reshape(-1, 1))\n",
    "\n",
    "# What is happening here???\n",
    "#x_train = x_train_pre\n",
    "#y_train_class = y_train_class_pre\n",
    "#y_train_reg = y_train_reg_pre\n",
    "\n",
    "# Start with only a small subset of the data\n",
    "indices = np.random.choice(len(x_train), size=10_000, replace=False)\n",
    "\n",
    "training_reg = ITGDataset(x_train[indices], y_train_class[indices], y_train_reg[indices])\n",
    "train_dataloader_reg = DataLoader(training_reg, batch_size=10_000, shuffle=False)\n",
    "\n",
    "# Actual number of unstable points\n",
    "print(len(y_train_class[indices][y_train_class[indices] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2370\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "out = classifier(torch.from_numpy(training_reg.X[training_reg.y == 1]).float())\n",
    "\n",
    "plt.hist(out.detach().numpy(), bins=100, density=True, histtype='step', label='predicted');\n",
    "\n",
    "rounded = np.round(out.detach().numpy()).astype(int)\n",
    "print(len(rounded[rounded == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123eef9",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**>>>>>>> remote**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "\n",
    "for X, y, z in train_dataloader_reg:\n",
    "    pred = classifier(X.float())\n",
    "    pred_class = torch.round(pred.squeeze()).detach().numpy()\n",
    "    pred_class = pred_class.astype(int)\n",
    "\n",
    "\n",
    "    #print number of points incorrectly classified\n",
    "    print(len(pred_class[pred_class != y.numpy()]))\n",
    "\n",
    "    # add data that returns one to a new dataset\n",
    "    training_reg_pass = ITGDataset(X[pred_class == 1], y[pred_class == 1], z[pred_class == 1])\n",
    "\n",
    "print(f\"Number of points passing classifier: {len(training_reg_pass)}\")\n",
    "\n",
    "training_reg_pass_dataloader = DataLoader(training_reg_pass, shuffle=False)\n",
    "\n",
    "# number of incorrect points passing classifier\n",
    "len(training_reg_pass.y[training_reg_pass.y == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model):\n",
    "    \"\"\"Function to enable the dropout layers during test-time\"\"\"\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "            m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.eval()\n",
    "enable_dropout(regressor)\n",
    "\n",
    "# evaluate model on training data 100 times and return points with largest uncertainty\n",
    "runs = []\n",
    "for i in tqdm(range(100)):\n",
    "    step_list = []\n",
    "    for step, (x, y, z) in enumerate(training_reg_pass_dataloader):\n",
    "\n",
    "        predictions = regressor(x.float()).detach().numpy()\n",
    "        step_list.append(predictions)\n",
    "\n",
    "    flattened_predictions = np.array(step_list).flatten()\n",
    "    runs.append(flattened_predictions)\n",
    "\n",
    "out_std = np.std(np.array(runs), axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(out_std, bins=100)\n",
    "\n",
    "# get indices of top 10% of points\n",
    "top_indices = np.argsort(out_std)[-int(len(out_std) * 0.5):]\n",
    "plt.figure()\n",
    "plt.hist(out_std[top_indices], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe train using some of the original training dataset and high variance points for balance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load regression model\n",
    "regressor = ITG_Regressor()\n",
    "regressor.load_state_dict(torch.load(\"regression_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# Put most uncertain points in a dataset\n",
    "training_reg_pass_dataloader2 = DataLoader(training_reg_pass, batch_size = len(training_reg_pass), shuffle=False)\n",
    "for step, (x, y, z) in enumerate(training_reg_pass_dataloader2):\n",
    "    uncertain_data = ITGDataset(x[top_indices], y[top_indices], z[top_indices].reshape(-1,1))\n",
    "\n",
    "tune_dataloader = DataLoader(uncertain_data, shuffle=False)\n",
    "\n",
    "print(len(uncertain_data.y[uncertain_data.y == 1]))\n",
    "print(len(uncertain_data.y[uncertain_data.y == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(regressor.parameters(), lr = 1e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "print(f\"Validation MSE loss before fine tuning using high variance points:\")\n",
    "regression_test_loop(reg_val_dataloader, regressor, MSE_loss)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    # Add in some of the training data to balance the fine tuning on the most uncertain points\n",
    "\n",
    "    #train_idx = np.random.choice(len(x_train_pre), size=len(top_indices), replace=False)\n",
    "    #tune_dataset.add(x_train_pre[train_idx], y_train_class_pre[train_idx], y_train_reg_pre[train_idx])\n",
    "\n",
    "    if t % 2 == 0:\n",
    "        regression_train_loop(reg_train_dataloader, regressor, MSE_loss, optimizer)\n",
    "\n",
    "    regression_train_loop(tune_dataloader, regressor, MSE_loss, optimizer)\n",
    "    \n",
    "    regression_test_loop(reg_val_dataloader, regressor, MSE_loss)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate uncertainty to see if it has reduced\n",
    "runs_after = []\n",
    "for i in tqdm(range(100)):\n",
    "    step_list = []\n",
    "    for step, (x, y, z) in enumerate(tune_dataloader):\n",
    "\n",
    "        predictions = regressor(x.float()).detach().numpy()\n",
    "        step_list.append(predictions)\n",
    "\n",
    "    flattened_predictions = np.array(step_list).flatten()\n",
    "    runs_after.append(flattened_predictions)\n",
    "\n",
    "out_std_after = np.std(np.array(runs_after), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(out_std_after, bins=50);\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(out_std[top_indices], out_std_after, s=2, alpha=1);\n",
    "\n",
    "plt.plot([0.0,2.5],[0,2.5], 'k--', label = 'y = x');\n",
    "plt.xlim(out_std[top_indices].min(), out_std[top_indices].max());\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat uncertainty calculation on full dataset to see if it has increased\n",
    "\n",
    "runs_full = []\n",
    "for i in tqdm(range(100)):\n",
    "    step_list = []\n",
    "    for step, (x, y, z) in enumerate(training_reg_pass_dataloader):\n",
    "\n",
    "        predictions = regressor(x.float()).detach().numpy()\n",
    "        step_list.append(predictions)\n",
    "\n",
    "    flattened_predictions = np.array(step_list).flatten()\n",
    "    runs_full.append(flattened_predictions)\n",
    "\n",
    "out_std_full = np.std(np.array(runs), axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(out_std_full, bins=100)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(out_std, out_std_full, s=2, alpha=1);\n",
    "plt.plot([0.0,2],[0,2], 'k--', label = 'y = x');\n",
    "plt.xlim(out_std.min(), out_std.max())\n",
    "plt.ylim(out_std_full.min(), out_std_full.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Turn into a Pipeline with Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_points(X, y, z, n_points):\n",
    "    \"\"\"\n",
    "    Selects n_points random points from the dataset.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(X), n_points, replace=False)\n",
    "\n",
    "    dataset = ITGDataset(X[indices], y[indices], z[indices])\n",
    "    dataloader = DataLoader(training_reg, batch_size=n_points, shuffle=True)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "def unstable_itg_points(dataloader, classifier):\n",
    "    \"\"\"\n",
    "    Returns a dataset of points that are unstable.\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    unstable_points = []\n",
    "    for step, (x, y, z) in enumerate(dataloader):\n",
    "        pred = classifier(x.float())\n",
    "        pred_class = torch.round(pred.squeeze()).detach().numpy()\n",
    "        pred_class = pred_class.astype(int)\n",
    "        # add data that returns one to a new dataset\n",
    "        unstable_points.append(pred_class == 1)\n",
    "\n",
    "        if step == 0:\n",
    "            unstable_dataset = ITGDataset(x[pred_class == 1], y[pred_class == 1], z[pred_class == 1])\n",
    "        else:\n",
    "            unstable_dataset.add(x[pred_class == 1], y[pred_class == 1], z[pred_class == 1])\n",
    "    \n",
    "\n",
    "    print(f\"Number of unstable points: {len(unstable_dataset)}\")\n",
    "    unstable_dataloader = DataLoader(unstable_dataset, shuffle=True)\n",
    "    return unstable_dataloader\n",
    "\n",
    "dataset, dataloader = select_random_points(x_train, y_train_class, y_train_reg, n_points=10_000)\n",
    "zz = unstable_itg_points(dataloader, classifier)\n",
    "\n",
    "for X, y, z in dataloader:\n",
    "    pred = classifier(X.float())\n",
    "    pred_class = torch.round(pred.squeeze()).detach().numpy()\n",
    "    pred_class = pred_class.astype(int)\n",
    "    # add data that returns one to a new dataset\n",
    "    dataloader_pass = ITGDataset(X[pred_class == 1], y[pred_class == 1], z[pred_class == 1])\n",
    "\n",
    "print(f\"Number of points passing classifier: {len(dataloader_pass)}\")\n",
    "\n",
    "# check two arrays are the same (even if not in the same order)\n",
    "assert np.array_equal(np.sort(torch.flatten(zz.dataset.X)), np.sort(torch.flatten(dataloader_pass.X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_uncertainty(dataloader, regressor, keep = 0.1):\n",
    "    \"\"\"\n",
    "    Calculates the uncertainty of the regressor on the points in the dataloader.\n",
    "    Returns the most uncertain points.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    regressor.eval()\n",
    "    enable_dropout(regressor)\n",
    "\n",
    "    # evaluate model on training data 100 times and return points with largest uncertainty\n",
    "    runs = []\n",
    "    for i in tqdm(range(100)):\n",
    "        step_list = []\n",
    "        for step, (x, y, z) in enumerate(dataloader):\n",
    "\n",
    "            predictions = regressor(x.float()).detach().numpy()\n",
    "            step_list.append(predictions)\n",
    "\n",
    "        flattened_predictions = np.array(step_list).flatten()\n",
    "        runs.append(flattened_predictions)\n",
    "\n",
    "    out_std = np.std(np.array(runs), axis=0)\n",
    "\n",
    "    top_indices = np.argsort(out_std)[-int(len(out_std) * keep):]\n",
    "\n",
    "    uncertain_dataset = ITGDataset(\n",
    "        dataloader.dataset.X[top_indices],\n",
    "        dataloader.dataset.y[top_indices], \n",
    "        dataloader.dataset.z[top_indices].reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "    uncertain_dataloader = DataLoader(uncertain_dataset, shuffle=True)\n",
    "\n",
    "    return uncertain_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62cd9ed28b67c4b9c4c30ea5a14880038440710307d8d20eddeed8b6c088e38a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
